#!/usr/bin/env python3
"""
PersonaPlex Web æµ‹è¯•ç•Œé¢
ä½¿ç”¨ Gradio åˆ›å»ºç®€æ˜“å‰ç«¯
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, MoshiForConditionalGeneration
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

# å…¨å±€å˜é‡
MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model_once():
    """åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    global model, processor
    
    if model is not None:
        return "âœ… æ¨¡å‹å·²åŠ è½½"
    
    try:
        # è®¤è¯
        auth_success = False
        if HF_TOKEN:
            if not HF_TOKEN.startswith('hf_'):
                print("âš ï¸  è­¦å‘Š: Token æ ¼å¼å¯èƒ½ä¸æ­£ç¡®ï¼ˆåº”ä»¥ 'hf_' å¼€å¤´ï¼‰")
            try:
                login(token=HF_TOKEN)
                auth_success = True
            except Exception as e:
                print(f"âš ï¸  Token è®¤è¯å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ token æˆ– token å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ CLI ç™»å½•
        if not auth_success:
            try:
                from huggingface_hub import whoami
                user_info = whoami()
                if user_info:
                    print(f"âœ… ä½¿ç”¨ huggingface-cli è®¤è¯: {user_info.get('name', 'Unknown')}")
                    auth_success = True
            except:
                pass
        
        if not auth_success:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®¤è¯æ–¹å¼")
            print("   è¯·è®¾ç½® HF_TOKEN æˆ–è¿è¡Œ: huggingface-cli login")
        
        print("ğŸ“¥ åŠ è½½å¤„ç†å™¨...")
        processor = AutoProcessor.from_pretrained(
            MODEL_ID,
            trust_remote_code=True
        )
        
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        try:
            model = MoshiForConditionalGeneration.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
        except:
            model = AutoModelForSpeechSeq2Seq.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
        
        model.eval()
        
        memory_info = ""
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_info = f"\næ˜¾å­˜ä½¿ç”¨: {memory_used:.2f} GB"
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼{memory_info}"
        
    except Exception as e:
        return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

def process_audio(audio_file, text_prompt, voice_prompt_file):
    """å¤„ç†éŸ³é¢‘è¾“å…¥å¹¶ç”Ÿæˆå“åº”"""
    global model, processor
    
    if model is None or processor is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼", None
    
    try:
        # å¤„ç†ç”¨æˆ·éŸ³é¢‘
        if audio_file is None:
            # å¦‚æœæ²¡æœ‰ä¸Šä¼ éŸ³é¢‘ï¼Œåˆ›å»ºé™éŸ³
            sample_rate = 24000
            duration = 1.0
            user_audio = np.zeros(int(sample_rate * duration), dtype=np.float32)
            user_sr = sample_rate
        else:
            # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘
            user_audio, user_sr = sf.read(audio_file)
            if len(user_audio.shape) > 1:
                user_audio = np.mean(user_audio, axis=1)
            
            # é‡é‡‡æ ·åˆ° 24kHz
            if user_sr != 24000:
                import librosa
                user_audio = librosa.resample(
                    user_audio,
                    orig_sr=user_sr,
                    target_sr=24000
                )
                user_sr = 24000
        
        # å¤„ç†æ–‡æœ¬æç¤º
        if not text_prompt or text_prompt.strip() == "":
            text_prompt = "You are a helpful AI assistant. Respond naturally and conversationally."
        
        # å¤„ç†è¯­éŸ³æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
        voice_prompt_audio = None
        if voice_prompt_file:
            voice_prompt_audio, _ = sf.read(voice_prompt_file)
            if len(voice_prompt_audio.shape) > 1:
                voice_prompt_audio = np.mean(voice_prompt_audio, axis=1)
        
        print(f"å¤„ç†è¾“å…¥: éŸ³é¢‘é•¿åº¦={len(user_audio)/user_sr:.2f}ç§’, æ–‡æœ¬='{text_prompt}'")
        
        # å‡†å¤‡è¾“å…¥
        inputs = processor(
            audio=user_audio,
            sampling_rate=user_sr,
            text=text_prompt,
            return_tensors="pt"
        )
        
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                 for k, v in inputs.items()}
        
        # ç”Ÿæˆè¾“å‡º
        print("æ‰§è¡Œæ¨ç†...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        # è§£ç æ–‡æœ¬è¾“å‡º
        if hasattr(processor, 'decode'):
            text_output = processor.decode(outputs[0], skip_special_tokens=True)
        else:
            text_output = "æ¨ç†å®Œæˆï¼ˆæ— æ³•è§£ç æ–‡æœ¬ï¼‰"
        
        # å°è¯•æå–éŸ³é¢‘è¾“å‡º
        output_audio = None
        if hasattr(outputs, 'audio_values'):
            output_audio = outputs.audio_values.cpu().numpy()
            if len(output_audio.shape) > 1:
                output_audio = output_audio[0] if output_audio.shape[0] == 1 else output_audio
        elif isinstance(outputs, dict) and 'audio_values' in outputs:
            output_audio = outputs['audio_values'].cpu().numpy()
        
        # å¦‚æœæ²¡æœ‰éŸ³é¢‘è¾“å‡ºï¼Œç”Ÿæˆä¸€ä¸ªå ä½éŸ³é¢‘
        if output_audio is None:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æç¤ºéŸ³
            sample_rate = 24000
            duration = 2.0
            output_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, int(sample_rate * duration)))
            output_audio = output_audio.astype(np.float32)
            text_output += "\n\nâš ï¸ æ³¨æ„: éŸ³é¢‘è¾“å‡ºä¸å¯ç”¨ï¼Œå·²ç”Ÿæˆå ä½éŸ³é¢‘"
        
        # ä¿å­˜è¾“å‡ºéŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        output_audio_path = "/tmp/personaplex_output.wav"
        sf.write(output_audio_path, output_audio, 24000)
        
        status = f"âœ… æ¨ç†å®Œæˆï¼\næ–‡æœ¬è¾“å‡º: {text_output[:100]}..."
        
        return output_audio_path, status, text_output
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        import traceback
        traceback.print_exc()
        return None, error_msg, None

def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯"""
    info = "## ç³»ç»Ÿä¿¡æ¯\n\n"
    
    if torch.cuda.is_available():
        info += f"**GPU**: {torch.cuda.get_device_name(0)}\n"
        info += f"**æ˜¾å­˜**: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\n"
        if model is not None:
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_reserved = torch.cuda.memory_reserved(0) / 1e9
            info += f"**å·²ç”¨æ˜¾å­˜**: {memory_used:.2f} GB / {memory_reserved:.2f} GB\n"
    else:
        info += "**è®¾å¤‡**: CPUï¼ˆä¸æ¨èï¼‰\n"
    
    info += f"\n**æ¨¡å‹çŠ¶æ€**: {'âœ… å·²åŠ è½½' if model is not None else 'âŒ æœªåŠ è½½'}\n"
    info += f"**æ¨¡å‹ ID**: {MODEL_ID}\n"
    
    return info

# åˆ›å»º Gradio ç•Œé¢
def create_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(title="PersonaPlex æµ‹è¯•ç•Œé¢", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ™ï¸ PersonaPlex æ¨¡å‹æµ‹è¯•ç•Œé¢
        
        è¿™æ˜¯ä¸€ä¸ªç”¨äºæµ‹è¯• NVIDIA PersonaPlex-7b-v1 æ¨¡å‹çš„ç®€æ˜“ Web ç•Œé¢ã€‚
        
        **ä½¿ç”¨è¯´æ˜:**
        1. ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
        2. ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œ24kHz WAV æ ¼å¼æ¨èï¼‰
        3. è¾“å…¥æ–‡æœ¬æç¤ºï¼ˆå®šä¹‰ AI çš„è§’è‰²å’Œé£æ ¼ï¼‰
        4. ç‚¹å‡»"ç”Ÿæˆå“åº”"æŒ‰é’®
        5. æŸ¥çœ‹æ–‡æœ¬è¾“å‡ºå’Œæ’­æ”¾éŸ³é¢‘è¾“å‡º
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### æ¨¡å‹æ§åˆ¶")
                load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
                model_status = gr.Textbox(
                    label="æ¨¡å‹çŠ¶æ€",
                    value="âŒ æ¨¡å‹æœªåŠ è½½",
                    interactive=False
                )
                system_info = gr.Markdown(get_system_info())
                
            with gr.Column(scale=2):
                gr.Markdown("### è¾“å…¥è®¾ç½®")
                
                audio_input = gr.Audio(
                    label="ç”¨æˆ·éŸ³é¢‘è¾“å…¥ (å¯é€‰)",
                    type="filepath",
                    sources=["upload", "microphone"],
                    format="wav"
                )
                
                text_prompt = gr.Textbox(
                    label="æ–‡æœ¬æç¤º (è§’è‰²è®¾å®š)",
                    placeholder="ä¾‹å¦‚: You are a friendly customer service agent. Be helpful and professional.",
                    value="You are a helpful AI assistant. Respond naturally and conversationally.",
                    lines=3
                )
                
                voice_prompt = gr.Audio(
                    label="å‚è€ƒè¯­éŸ³æç¤º (å¯é€‰ï¼Œç”¨äºæ§åˆ¶éŸ³è‰²)",
                    type="filepath",
                    sources=["upload"],
                    format="wav"
                )
                
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆå“åº”", variant="primary", size="lg")
        
        with gr.Row():
            with gr.Column():
                gr.Markdown("### è¾“å‡ºç»“æœ")
                output_audio = gr.Audio(
                    label="AI è¯­éŸ³è¾“å‡º",
                    type="filepath",
                    format="wav"
                )
                output_text = gr.Textbox(
                    label="AI æ–‡æœ¬è¾“å‡º",
                    lines=5,
                    interactive=False
                )
                status_output = gr.Textbox(
                    label="çŠ¶æ€ä¿¡æ¯",
                    lines=3,
                    interactive=False
                )
        
        # äº‹ä»¶ç»‘å®š
        load_btn.click(
            fn=load_model_once,
            outputs=model_status
        ).then(
            fn=get_system_info,
            outputs=system_info
        )
        
        generate_btn.click(
            fn=process_audio,
            inputs=[audio_input, text_prompt, voice_prompt],
            outputs=[output_audio, status_output, output_text]
        )
        
        gr.Markdown("""
        ---
        ### ğŸ“ æç¤º
        
        - **éŸ³é¢‘æ ¼å¼**: æ¨èä½¿ç”¨ 24kHz WAV æ ¼å¼
        - **æ–‡æœ¬æç¤º**: ç”¨è‹±æ–‡æè¿° AI çš„è§’è‰²ã€æ€§æ ¼å’Œè¯´è¯é£æ ¼
        - **é¦–æ¬¡åŠ è½½**: æ¨¡å‹è¾ƒå¤§ï¼Œé¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦ 2-5 åˆ†é’Ÿ
        - **æ˜¾å­˜è¦æ±‚**: è‡³å°‘éœ€è¦ 16GB VRAMï¼ˆæ¨è 24GB+ï¼‰
        """)
    
    return demo

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("å¯åŠ¨ PersonaPlex Web ç•Œé¢")
    print("="*60)
    
    if not HF_TOKEN:
        print("âš ï¸  è­¦å‘Š: HF_TOKEN æœªè®¾ç½®")
        print("   æŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")
    
    demo = create_interface()
    
    # å¯åŠ¨æœåŠ¡å™¨
    # åœ¨ RunPod ä¸Šï¼Œä½¿ç”¨ share=False å¹¶ç»‘å®šåˆ° 0.0.0.0
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # Gradio é»˜è®¤ç«¯å£
        share=False,            # ä¸åˆ›å»ºå…¬å…±é“¾æ¥ï¼ˆRunPod æœ‰è‡ªå·±çš„ URLï¼‰
        show_error=True
    )

if __name__ == "__main__":
    main()

