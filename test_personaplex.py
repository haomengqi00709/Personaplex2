#!/usr/bin/env python3
"""
PersonaPlex æ¨¡å‹æµ‹è¯•è„šæœ¬
é€‚ç”¨äº RunPod GPU ç¯å¢ƒçš„æœ€ä½é…ç½®æµ‹è¯•
"""

import os
import torch
import yaml
import soundfile as sf
import numpy as np
from pathlib import Path
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, MoshiForConditionalGeneration, MoshiProcessor
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")


class PersonaPlexTester:
    """PersonaPlex æ¨¡å‹æµ‹è¯•ç±»"""
    
    def __init__(self, config_path="config.yaml"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config = self.load_config(config_path)
        self.device = torch.device(self.config["model"]["device"])
        self.model = None
        self.processor = None
        
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    def load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def check_huggingface_auth(self):
        """æ£€æŸ¥ Hugging Face è®¤è¯"""
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° HF_TOKEN ç¯å¢ƒå˜é‡")
            print("è¯·è®¾ç½®: export HF_TOKEN=your_token_here")
            print("æˆ–åœ¨ RunPod çš„ç¯å¢ƒå˜é‡ä¸­è®¾ç½®")
            return False
        
        try:
            login(token=hf_token)
            print("âœ… Hugging Face è®¤è¯æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ Hugging Face è®¤è¯å¤±è´¥: {e}")
            return False
    
    def load_model(self):
        """åŠ è½½ PersonaPlex æ¨¡å‹"""
        print("\nğŸ“¥ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        model_config = self.config["model"]
        model_id = model_config["model_id"]
        
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
            cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
            model_path = cache_dir / f"models--{model_id.replace('/', '--')}"
            
            print(f"æ¨¡å‹ ID: {model_id}")
            print(f"ä½¿ç”¨æ•°æ®ç±»å‹: {model_config['torch_dtype']}")
            
            # åŠ è½½å¤„ç†å™¨ï¼ˆPersonaPlex åŸºäº Moshiï¼Œä¼˜å…ˆä½¿ç”¨ MoshiProcessorï¼‰
            print("åŠ è½½å¤„ç†å™¨...")
            try:
                # é¦–å…ˆå°è¯• MoshiProcessor
                self.processor = MoshiProcessor.from_pretrained(
                    model_id,
                    trust_remote_code=True
                )
                print("âœ… ä½¿ç”¨ MoshiProcessor åŠ è½½æˆåŠŸ")
            except Exception as e1:
                print(f"âš ï¸  MoshiProcessor å¤±è´¥: {e1}")
                print("   å°è¯•ä½¿ç”¨ AutoProcessor...")
                try:
                    self.processor = AutoProcessor.from_pretrained(
                        model_id,
                        trust_remote_code=True
                    )
                    print("âœ… ä½¿ç”¨ AutoProcessor åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    print(f"âŒ AutoProcessor ä¹Ÿå¤±è´¥: {e2}")
                    self.processor = None
            
            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ float16 é™ä½æ˜¾å­˜ï¼‰
            print("åŠ è½½æ¨¡å‹æƒé‡...")
            torch_dtype = getattr(torch, model_config["torch_dtype"])
            
            # å°è¯•ä½¿ç”¨ MoshiForConditionalGenerationï¼ˆåŸºäº Moshi æ¶æ„ï¼‰
            try:
                self.model = MoshiForConditionalGeneration.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype,
                    device_map="auto",
                    low_cpu_mem_usage=model_config["low_cpu_mem_usage"],
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"âš ï¸  ä½¿ç”¨ MoshiForConditionalGeneration å¤±è´¥ï¼Œå°è¯• AutoModel: {e}")
                # å›é€€åˆ° AutoModel
                self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
                    model_id,
                    torch_dtype=torch_dtype,
                    device_map="auto",
                    low_cpu_mem_usage=model_config["low_cpu_mem_usage"],
                    trust_remote_code=True
                )
            
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(0) / 1e9
                memory_reserved = torch.cuda.memory_reserved(0) / 1e9
                print(f"æ˜¾å­˜ä½¿ç”¨: {memory_allocated:.2f} GB / {memory_reserved:.2f} GB")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥æ˜¯å¦å·²è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")
            print("2. ç¡®è®¤å·²æ¥å—æ¨¡å‹è®¸å¯åè®®: https://huggingface.co/nvidia/personaplex-7b-v1")
            print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ Hugging Face è®¿é—®")
            return False
    
    def prepare_audio(self, audio_path):
        """å‡†å¤‡éŸ³é¢‘è¾“å…¥ï¼ˆè½¬æ¢ä¸º 24kHzï¼‰"""
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        # è¯»å–éŸ³é¢‘
        audio, sr = sf.read(audio_path)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sr != self.config["audio"]["sample_rate"]:
            import librosa
            audio = librosa.resample(
                audio, 
                orig_sr=sr, 
                target_sr=self.config["audio"]["sample_rate"]
            )
        
        return audio, self.config["audio"]["sample_rate"]
    
    def test_inference(self, audio_path=None, text_prompt=None, voice_prompt_path=None):
        """æµ‹è¯•æ¨ç†"""
        print("\nğŸ§ª å¼€å§‹æ¨ç†æµ‹è¯•...")
        
        if self.model is None or self.processor is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
            return None
        
        # å‡†å¤‡è¾“å…¥
        if audio_path:
            print(f"å¤„ç†éŸ³é¢‘è¾“å…¥: {audio_path}")
            audio, sr = self.prepare_audio(audio_path)
        else:
            # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘ï¼ˆé™éŸ³æˆ–ç®€å•æ³¢å½¢ï¼‰
            print("ä½¿ç”¨é»˜è®¤æµ‹è¯•éŸ³é¢‘ï¼ˆ1ç§’é™éŸ³ï¼‰")
            duration = 1.0
            sr = self.config["audio"]["sample_rate"]
            audio = np.zeros(int(sr * duration))
        
        # å‡†å¤‡æ–‡æœ¬æç¤º
        if text_prompt is None:
            text_prompt = "You are a helpful AI assistant. Respond naturally and conversationally."
        
        print(f"æ–‡æœ¬æç¤º: {text_prompt}")
        
        try:
            # å¤„ç†è¾“å…¥
            if self.processor:
                inputs = self.processor(
                    audio=audio,
                    sampling_rate=sr,
                    text=text_prompt,
                    return_tensors="pt"
                )
                # ç§»åŠ¨åˆ°è®¾å¤‡
                inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                         for k, v in inputs.items()}
            else:
                # å¦‚æœæ²¡æœ‰ processorï¼Œæ‰‹åŠ¨å‡†å¤‡è¾“å…¥
                print("âš ï¸  ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥å‡†å¤‡...")
                # å°†éŸ³é¢‘è½¬æ¢ä¸º tensor
                if isinstance(audio, np.ndarray):
                    audio_tensor = torch.from_numpy(audio).float().unsqueeze(0).to(self.device)
                else:
                    audio_tensor = audio.to(self.device) if hasattr(audio, 'to') else torch.tensor(audio).to(self.device)
                
                inputs = {
                    'audio': audio_tensor,
                    'text': text_prompt
                }
            
            # æ¨ç†
            print("æ‰§è¡Œæ¨ç†...")
            with torch.no_grad():
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„ç”Ÿæˆæ–¹æ³•
                if hasattr(self.model, 'generate'):
                    if 'input_ids' in inputs:
                        outputs = self.model.generate(
                            input_ids=inputs.get('input_ids'),
                            audio_codes=inputs.get('audio_codes'),
                            max_new_tokens=self.config["inference"]["max_new_tokens"],
                            temperature=self.config["inference"]["temperature"],
                            top_p=self.config["inference"]["top_p"],
                            do_sample=self.config["inference"]["do_sample"]
                        )
                    else:
                        # å°è¯•ä½¿ç”¨æ¨¡å‹çš„ forward æ–¹æ³•
                        outputs = self.model(**inputs)
                else:
                    outputs = self.model(**inputs)
            
            # è§£ç è¾“å‡º
            if self.processor and hasattr(self.processor, 'decode'):
                # è·å–æ–‡æœ¬è¾“å‡º
                if isinstance(outputs, torch.Tensor):
                    text_output = self.processor.decode(outputs[0], skip_special_tokens=True)
                else:
                    text_output = str(outputs)
                print(f"\nâœ… æ–‡æœ¬è¾“å‡º: {text_output}")
                
                # è·å–éŸ³é¢‘è¾“å‡ºï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                if hasattr(outputs, 'audio_values'):
                    audio_output = outputs.audio_values.cpu().numpy()
                    return text_output, audio_output
                
                return text_output, None
            else:
                print(f"\nâœ… è¾“å‡º: {outputs}")
                return outputs, None
                
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def test_basic_functionality(self):
        """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
        print("\n" + "="*50)
        print("åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        print("="*50)
        
        # æµ‹è¯• 1: æ¨¡å‹åŠ è½½
        print("\n[æµ‹è¯• 1] æ¨¡å‹åŠ è½½")
        if not self.load_model():
            return False
        
        # æµ‹è¯• 2: ç®€å•æ¨ç†
        print("\n[æµ‹è¯• 2] ç®€å•æ¨ç†æµ‹è¯•")
        result = self.test_inference(
            text_prompt="Say hello in a friendly way."
        )
        
        if result:
            print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("="*50)
    print("PersonaPlex æ¨¡å‹æµ‹è¯•")
    print("="*50)
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = PersonaPlexTester()
    
    # æ£€æŸ¥è®¤è¯
    if not tester.check_huggingface_auth():
        print("\nâš ï¸  ç»§ç»­å°è¯•åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå·²è®¤è¯ï¼‰...")
    
    # è¿è¡ŒåŸºç¡€æµ‹è¯•
    success = tester.test_basic_functionality()
    
    if success:
        print("\n" + "="*50)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("="*50)
    else:
        print("\n" + "="*50)
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("="*50)


if __name__ == "__main__":
    main()

