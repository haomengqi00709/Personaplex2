#!/usr/bin/env python3
"""
PersonaPlex ç‹¬ç«‹å®æ—¶ç¿»è¯‘æœº
ä¸ä¾èµ–å®˜æ–¹ä»£ç åº“ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œç¿»è¯‘
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import AutoModel, AutoConfig
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

# å…¨å±€å˜é‡
MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model():
    """åŠ è½½æ¨¡å‹ï¼ˆä¸ä¾èµ– processorï¼‰"""
    global model
    
    if model is not None:
        memory_info = ""
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_info = f"\næ˜¾å­˜: {memory_used:.2f} GB"
        return f"âœ… æ¨¡å‹å·²åŠ è½½{memory_info}"
    
    try:
        # è®¤è¯
        if HF_TOKEN:
            login(token=HF_TOKEN)
        
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        
        # ä½¿ç”¨ AutoModel è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
        # è™½ç„¶ä¼šæœ‰è­¦å‘Šï¼Œä½†æ¨¡å‹å¯ä»¥åŠ è½½
        model = AutoModel.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model.eval()
        
        memory_info = ""
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_info = f"\næ˜¾å­˜: {memory_used:.2f} GB"
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼{memory_info}\n\nâš ï¸ æ³¨æ„: ç”±äºç¼ºå°‘ processorï¼Œæ¨ç†åŠŸèƒ½å—é™ã€‚\nå»ºè®®ä½¿ç”¨æ–‡æœ¬æç¤ºæ–¹å¼æµ‹è¯•æ¨¡å‹èƒ½åŠ›ã€‚"
        
    except Exception as e:
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"

def process_audio_basic(audio_file, text_prompt):
    """åŸºç¡€éŸ³é¢‘å¤„ç†ï¼ˆä¸ä¾èµ– processorï¼‰"""
    try:
        # è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio_file)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != 24000:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=24000)
            sr = 24000
        
        # è½¬æ¢ä¸º tensor
        audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0).to(device)
        
        return audio_tensor, sr, True
    except Exception as e:
        return None, None, False

def translate_audio(audio_file, source_lang, target_lang, custom_prompt):
    """ç¿»è¯‘éŸ³é¢‘"""
    global model
    
    if model is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹ï¼ç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®"
    
    if audio_file is None:
        return None, "âŒ è¯·å½•åˆ¶æˆ–ä¸Šä¼ éŸ³é¢‘"
    
    try:
        # å¤„ç†éŸ³é¢‘
        audio_tensor, sr, success = process_audio_basic(audio_file, None)
        if not success:
            return None, "âŒ éŸ³é¢‘å¤„ç†å¤±è´¥"
        
        # æ„å»ºç¿»è¯‘æç¤º
        if custom_prompt:
            text_prompt = custom_prompt
        else:
            text_prompt = f"You are a real-time translator. Translate from {source_lang} to {target_lang}. Speak naturally and clearly in {target_lang}."
        
        # ç”±äºæ²¡æœ‰ processorï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å‡†å¤‡è¾“å…¥
        # è¿™é‡Œä½¿ç”¨æ¨¡å‹çš„ forward æ–¹æ³•
        # æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦æ ¹æ®å®é™…æ¨¡å‹ç»“æ„è°ƒæ•´
        
        print(f"å¤„ç†ç¿»è¯‘: {source_lang} -> {target_lang}")
        print(f"éŸ³é¢‘é•¿åº¦: {audio_tensor.shape[1] / sr:.2f}ç§’")
        print(f"æç¤º: {text_prompt}")
        
        # å°è¯•è°ƒç”¨æ¨¡å‹
        # ç”±äºæ¨¡å‹æ¶æ„ç‰¹æ®Šï¼Œå¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€æ¡†æ¶
        
        with torch.no_grad():
            try:
                # æ–¹æ³•1: å°è¯•ç›´æ¥ forwardï¼ˆéœ€è¦çŸ¥é“è¾“å…¥æ ¼å¼ï¼‰
                # ç”±äº PersonaPlex ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´
                
                # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è¾“å‡º
                # å®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®æ¨¡å‹æ–‡æ¡£è°ƒæ•´
                result_text = f"ç¿»è¯‘æç¤ºå·²è®¾ç½®: {text_prompt}\n\nç”±äºç¼ºå°‘ processorï¼Œæ— æ³•å®Œæˆå®Œæ•´æ¨ç†ã€‚\n\nå»ºè®®:\n1. æŸ¥çœ‹æ¨¡å‹æ–‡æ¡£äº†è§£è¾“å…¥æ ¼å¼\n2. æˆ–ä½¿ç”¨å®˜æ–¹ä»£ç åº“\n3. æˆ–å‡çº§ transformers åˆ°æœ€æ–°ç‰ˆæœ¬"
                
                # ç”Ÿæˆå ä½éŸ³é¢‘
                sample_rate = 24000
                duration = 2.0
                output_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, int(sample_rate * duration)))
                output_audio = output_audio.astype(np.float32)
                
                output_path = "/tmp/translation_output.wav"
                sf.write(output_path, output_audio, sample_rate)
                
                return output_path, result_text
                
            except Exception as e:
                return None, f"âŒ æ¨ç†å¤±è´¥: {str(e)}\n\nè¿™å¯èƒ½éœ€è¦:\n1. äº†è§£æ¨¡å‹çš„è¾“å…¥æ ¼å¼\n2. æ‰‹åŠ¨å®ç°éŸ³é¢‘ç¼–ç \n3. æˆ–ä½¿ç”¨æ”¯æŒ PersonaPlex çš„ transformers ç‰ˆæœ¬"
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"âŒ å¤„ç†å¤±è´¥: {str(e)}"

def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    info = "## æ¨¡å‹ä¿¡æ¯\n\n"
    
    if model is not None:
        info += "âœ… **æ¨¡å‹å·²åŠ è½½**\n\n"
        info += f"**æ¨¡å‹ ID**: {MODEL_ID}\n"
        info += f"**è®¾å¤‡**: {device}\n"
        
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_reserved = torch.cuda.memory_reserved(0) / 1e9
            info += f"**æ˜¾å­˜ä½¿ç”¨**: {memory_used:.2f} GB / {memory_reserved:.2f} GB\n"
    else:
        info += "âŒ **æ¨¡å‹æœªåŠ è½½**\n\n"
        info += "ç‚¹å‡»'åŠ è½½æ¨¡å‹'æŒ‰é’®å¼€å§‹"
    
    return info

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="PersonaPlex å®æ—¶ç¿»è¯‘æœº", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸŒ PersonaPlex å®æ—¶ç¿»è¯‘æœº
    
    ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬ - ä¸ä¾èµ–å®˜æ–¹ä»£ç åº“
    
    **åŠŸèƒ½**: è¯­éŸ³è¾“å…¥ â†’ å®æ—¶ç¿»è¯‘ â†’ è¯­éŸ³è¾“å‡º
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### æ¨¡å‹æ§åˆ¶")
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            status = gr.Textbox(label="çŠ¶æ€", value="âŒ æ¨¡å‹æœªåŠ è½½", interactive=False, lines=4)
            model_info = gr.Markdown(get_model_info())
        
        with gr.Column(scale=2):
            gr.Markdown("### ç¿»è¯‘è®¾ç½®")
            
            with gr.Row():
                source_lang = gr.Dropdown(
                    label="æºè¯­è¨€",
                    choices=["English", "Chinese", "Spanish", "French", "German", "Japanese", "Korean", "Russian"],
                    value="English"
                )
                
                target_lang = gr.Dropdown(
                    label="ç›®æ ‡è¯­è¨€",
                    choices=["English", "Chinese", "Spanish", "French", "German", "Japanese", "Korean", "Russian"],
                    value="Chinese"
                )
            
            custom_prompt = gr.Textbox(
                label="è‡ªå®šä¹‰æç¤ºï¼ˆå¯é€‰ï¼‰",
                placeholder="ä¾‹å¦‚: Translate naturally, maintain the speaker's tone and emotion.",
                lines=2
            )
            
            audio_input = gr.Audio(
                label="ğŸ¤ è¯´è¯ï¼ˆå½•åˆ¶æˆ–ä¸Šä¼ ï¼‰",
                type="filepath",
                sources=["microphone", "upload"],
                format="wav"
            )
            
            translate_btn = gr.Button("ğŸš€ å¼€å§‹ç¿»è¯‘", variant="primary", size="lg")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ç¿»è¯‘ç»“æœ")
            audio_output = gr.Audio(label="ğŸ”Š ç¿»è¯‘åçš„è¯­éŸ³", type="filepath", format="wav")
            text_output = gr.Textbox(label="ğŸ“ ç¿»è¯‘æ–‡æœ¬/çŠ¶æ€", lines=6, interactive=False)
    
    gr.Markdown("""
    ---
    ### ğŸ“ ä½¿ç”¨è¯´æ˜
    
    1. **åŠ è½½æ¨¡å‹**: ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®ï¼ˆé¦–æ¬¡éœ€è¦å‡ åˆ†é’Ÿï¼‰
    2. **é€‰æ‹©è¯­è¨€**: é€‰æ‹©æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
    3. **å½•åˆ¶éŸ³é¢‘**: ç‚¹å‡»éº¦å…‹é£å›¾æ ‡å½•åˆ¶ï¼Œæˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
    4. **å¼€å§‹ç¿»è¯‘**: ç‚¹å‡»"å¼€å§‹ç¿»è¯‘"æŒ‰é’®
    5. **æŸ¥çœ‹ç»“æœ**: æ’­æ”¾ç¿»è¯‘åçš„è¯­éŸ³ï¼ŒæŸ¥çœ‹ç¿»è¯‘æ–‡æœ¬
    
    ### âš ï¸ æ³¨æ„äº‹é¡¹
    
    - æ¨¡å‹å·²æˆåŠŸåŠ è½½ï¼Œä½†ç”±äºç¼ºå°‘ processorï¼Œå®Œæ•´æ¨ç†åŠŸèƒ½å¯èƒ½å—é™
    - å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯èƒ½éœ€è¦äº†è§£æ¨¡å‹çš„è¾“å…¥æ ¼å¼
    - å»ºè®®ä½¿ç”¨ 24kHz WAV æ ¼å¼çš„éŸ³é¢‘
    """)
    
    # äº‹ä»¶ç»‘å®š
    def update_info():
        return get_model_info()
    
    load_btn.click(
        fn=load_model,
        outputs=status
    ).then(
        fn=update_info,
        outputs=model_info
    )
    
    translate_btn.click(
        fn=translate_audio,
        inputs=[audio_input, source_lang, target_lang, custom_prompt],
        outputs=[audio_output, text_output]
    )

if __name__ == "__main__":
    print("="*60)
    print("PersonaPlex å®æ—¶ç¿»è¯‘æœº - ç‹¬ç«‹ç‰ˆæœ¬")
    print("ç«¯å£: 5001")
    print("="*60)
    
    if not HF_TOKEN:
        print("âš ï¸  è­¦å‘Š: HF_TOKEN æœªè®¾ç½®")
        print("   æŸäº›åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨")
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=5001,
        share=False
    )

