#!/usr/bin/env python3
"""
PersonaPlex ç®€å•å®æ—¶è¯­éŸ³å¯¹è¯æµ‹è¯•ç•Œé¢
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, MoshiForConditionalGeneration
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

# å…¨å±€å˜é‡
MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global model, processor
    
    if model is not None:
        return "âœ… æ¨¡å‹å·²åŠ è½½"
    
    try:
        # è®¤è¯
        if HF_TOKEN:
            login(token=HF_TOKEN)
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ MoshiForConditionalGenerationï¼‰
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model = MoshiForConditionalGeneration.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        model.eval()
        
        # å°è¯•åŠ è½½ processorï¼ˆå¦‚æœå¤±è´¥ä¹Ÿæ²¡å…³ç³»ï¼‰
        try:
            processor = AutoProcessor.from_pretrained(
                MODEL_ID,
                trust_remote_code=True
            )
        except:
            processor = None
            print("âš ï¸  Processor ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½")
        
        memory_info = ""
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_info = f"\næ˜¾å­˜: {memory_used:.2f} GB"
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼{memory_info}"
        
    except Exception as e:
        return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"

def chat(audio, text_prompt):
    """å¤„ç†è¯­éŸ³è¾“å…¥å¹¶ç”Ÿæˆå“åº”"""
    global model, processor
    
    if model is None:
        return None, "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if processor is None:
        return None, "âŒ Processor ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨ç†ã€‚\n\nå»ºè®®ä½¿ç”¨å®˜æ–¹ PersonaPlex ä»£ç åº“ã€‚"
    
    if audio is None:
        return None, "âŒ è¯·å½•åˆ¶æˆ–ä¸Šä¼ éŸ³é¢‘"
    
    try:
        # è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != 24000:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=24000)
            sr = 24000
        
        # è®¾ç½®æ–‡æœ¬æç¤º
        if not text_prompt or text_prompt.strip() == "":
            text_prompt = "You are a helpful AI assistant."
        
        # å¤„ç†è¾“å…¥
        inputs = processor(
            audio=audio_data,
            sampling_rate=sr,
            text=text_prompt,
            return_tensors="pt"
        )
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                 for k, v in inputs.items()}
        
        # ç”Ÿæˆå“åº”
        print("ç”Ÿæˆå“åº”...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True
            )
        
        # è§£ç æ–‡æœ¬
        if hasattr(processor, 'decode'):
            text_output = processor.decode(outputs[0], skip_special_tokens=True)
        else:
            text_output = "å“åº”å·²ç”Ÿæˆ"
        
        # å°è¯•æå–éŸ³é¢‘è¾“å‡º
        output_audio = None
        if hasattr(outputs, 'audio_values'):
            output_audio = outputs.audio_values.cpu().numpy()
        elif isinstance(outputs, dict) and 'audio_values' in outputs:
            output_audio = outputs['audio_values'].cpu().numpy()
        
        # å¦‚æœæ²¡æœ‰éŸ³é¢‘è¾“å‡ºï¼Œç”Ÿæˆå ä½éŸ³é¢‘
        if output_audio is None:
            sample_rate = 24000
            duration = 2.0
            output_audio = np.sin(2 * np.pi * 440 * np.linspace(0, duration, int(sample_rate * duration)))
            output_audio = output_audio.astype(np.float32)
            text_output += "\n\nâš ï¸ éŸ³é¢‘è¾“å‡ºä¸å¯ç”¨ï¼ˆå·²ç”Ÿæˆå ä½éŸ³é¢‘ï¼‰"
        
        # ä¿å­˜è¾“å‡ºéŸ³é¢‘
        output_path = "/tmp/personaplex_response.wav"
        sf.write(output_path, output_audio, 24000)
        
        return output_path, text_output
        
    except Exception as e:
        error_msg = f"âŒ é”™è¯¯: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return None, error_msg

# åˆ›å»ºç®€å•ç•Œé¢
with gr.Blocks(title="PersonaPlex è¯­éŸ³å¯¹è¯", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ™ï¸ PersonaPlex å®æ—¶è¯­éŸ³å¯¹è¯æµ‹è¯•
    
    ç®€å•æµ‹è¯•ç•Œé¢ï¼šè¯´è¯ â†’ AI å›å¤
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary", size="lg")
            status = gr.Textbox(label="çŠ¶æ€", value="âŒ æ¨¡å‹æœªåŠ è½½", interactive=False)
        
        with gr.Column(scale=2):
            text_prompt = gr.Textbox(
                label="è§’è‰²è®¾å®šï¼ˆå¯é€‰ï¼‰",
                value="You are a helpful AI assistant.",
                placeholder="ä¾‹å¦‚: You are a friendly assistant.",
                lines=2
            )
    
    with gr.Row():
        audio_input = gr.Audio(
            label="ğŸ¤ è¯´è¯ï¼ˆç‚¹å‡»å½•åˆ¶ï¼‰",
            type="filepath",
            sources=["microphone"],
            format="wav"
        )
    
    chat_btn = gr.Button("ğŸš€ å‘é€", variant="primary", size="lg")
    
    with gr.Row():
        audio_output = gr.Audio(label="ğŸ”Š AI å›å¤", type="filepath", format="wav")
        text_output = gr.Textbox(label="ğŸ“ æ–‡æœ¬å›å¤", lines=3, interactive=False)
    
    # äº‹ä»¶
    load_btn.click(fn=load_model, outputs=status)
    chat_btn.click(fn=chat, inputs=[audio_input, text_prompt], outputs=[audio_output, text_output])

if __name__ == "__main__":
    print("="*60)
    print("å¯åŠ¨ PersonaPlex ç®€å•è¯­éŸ³å¯¹è¯ç•Œé¢")
    print("ç«¯å£: 5001")
    print("="*60)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=5001,
        share=False
    )

