#!/usr/bin/env python3
"""
PersonaPlex ç®€å•è¯­éŸ³å¯¹è¯ - ä½¿ç”¨å®˜æ–¹æ­£ç¡®æ–¹å¼
åªæœ‰ä¸€ä¸ªæŒ‰é’®ï¼šè¯´è¯
"""

import os
import sys
import torch
import numpy as np
import soundfile as sf
import gradio as gr
import warnings
warnings.filterwarnings("ignore")

# å°è¯•å¤šç§æ–¹å¼å¯¼å…¥å®˜æ–¹ moshi åŒ…
OFFICIAL_MOSHI_AVAILABLE = False
moshi_paths = [
    os.path.join(os.path.dirname(__file__), 'personaplex', 'moshi'),
    os.path.join(os.path.dirname(__file__), '..', 'personaplex', 'moshi'),
    '/workspace/personaplex/moshi',
]

for moshi_path in moshi_paths:
    if os.path.exists(moshi_path):
        sys.path.insert(0, moshi_path)
        print(f"[INFO] æ‰¾åˆ° moshi åŒ…è·¯å¾„: {moshi_path}")
        break

try:
    from moshi.models import loaders, LMGen, MimiModel
    from moshi.models.lm import load_audio, _iterate_audio, encode_from_sphn
    from moshi.client_utils import make_log
    OFFICIAL_MOSHI_AVAILABLE = True
    print("[INFO] âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹ moshi åŒ…")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥å®˜æ–¹ moshi åŒ…: {e}")
    OFFICIAL_MOSHI_AVAILABLE = False

MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
device = "cuda" if torch.cuda.is_available() else "cpu"

# éªŒè¯ Token æ˜¯å¦è®¾ç½®
if not HF_TOKEN:
    print("âš ï¸ è­¦å‘Š: æœªè®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")
else:
    print(f"[INFO] HF_TOKEN å·²è®¾ç½® (é•¿åº¦: {len(HF_TOKEN)})")
    try:
        from huggingface_hub import login
        login(token=HF_TOKEN, add_to_git_credential=False)
        print("[INFO] âœ… é¢„è®¤è¯æˆåŠŸ")
    except Exception as e:
        print(f"[INFO] âš ï¸ é¢„è®¤è¯è­¦å‘Š: {e}")

# å…¨å±€å˜é‡
mimi = None
other_mimi = None
lm = None
lm_gen = None
text_tokenizer = None
model_status = "æœªåŠ è½½"

def load_model():
    """åŠ è½½æ¨¡å‹ - ä½¿ç”¨å®˜æ–¹æ–¹å¼"""
    global mimi, other_mimi, lm, lm_gen, text_tokenizer, model_status
    
    if not OFFICIAL_MOSHI_AVAILABLE:
        return "âŒ å®˜æ–¹ moshi åŒ…ä¸å¯ç”¨"
    
    if mimi is not None:
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        return f"âœ… æ¨¡å‹å·²åŠ è½½ (æ˜¾å­˜: {mem:.2f} GB)"
    
    try:
        print("[INFO] å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # 0. ç¡®ä¿ Hugging Face è®¤è¯
        if HF_TOKEN:
            from huggingface_hub import login
            try:
                login(token=HF_TOKEN, add_to_git_credential=False)
                print("[INFO] âœ… Hugging Face è®¤è¯æˆåŠŸ")
            except Exception as e:
                print(f"[INFO] âš ï¸ è®¤è¯è­¦å‘Š: {e}")
        
        # 1. åŠ è½½ Mimi ç¼–ç å™¨/è§£ç å™¨
        print("[INFO] åŠ è½½ Mimi...")
        from huggingface_hub import hf_hub_download
        mimi_weight = hf_hub_download(MODEL_ID, loaders.MIMI_NAME, token=HF_TOKEN)
        mimi = loaders.get_mimi(mimi_weight, device)
        other_mimi = loaders.get_mimi(mimi_weight, device)
        print("[INFO] Mimi åŠ è½½å®Œæˆ")
        
        # 2. åŠ è½½ tokenizer
        print("[INFO] åŠ è½½ tokenizer...")
        import sentencepiece
        tokenizer_path = hf_hub_download(MODEL_ID, loaders.TEXT_TOKENIZER_NAME, token=HF_TOKEN)
        text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)
        print("[INFO] Tokenizer åŠ è½½å®Œæˆ")
        
        # 3. åŠ è½½ Moshi LM
        print("[INFO] åŠ è½½ Moshi LM...")
        moshi_weight = hf_hub_download(MODEL_ID, loaders.MOSHI_NAME, token=HF_TOKEN)
        lm = loaders.get_moshi_lm(moshi_weight, device=device, cpu_offload=False)
        lm.eval()
        print("[INFO] Moshi LM åŠ è½½å®Œæˆ")
        
        # 4. åˆ›å»º LMGen
        print("[INFO] åˆ›å»º LMGen...")
        frame_size = int(mimi.sample_rate / mimi.frame_rate)
        lm_gen = LMGen(
            lm,
            audio_silence_frame_cnt=int(0.5 * mimi.frame_rate),
            sample_rate=mimi.sample_rate,
            device=device,
            frame_rate=mimi.frame_rate,
            save_voice_prompt_embeddings=False,
            use_sampling=True,
            temp=0.8,
            temp_text=0.7,
            top_k=250,
            top_k_text=25,
        )
        
        # è®¾ç½®æµå¼æ¨¡å¼
        mimi.streaming_forever(1)
        other_mimi.streaming_forever(1)
        lm_gen.streaming_forever(1)
        
        # 5. Warmup
        print("[INFO] é¢„çƒ­æ¨¡å‹...")
        for _ in range(4):
            chunk = torch.zeros(1, 1, frame_size, dtype=torch.float32, device=device)
            codes = mimi.encode(chunk)
            _ = other_mimi.encode(chunk)
            for c in range(codes.shape[-1]):
                tokens = lm_gen.step(codes[:, :, c : c + 1])
                if tokens is not None:
                    _ = mimi.decode(tokens[:, 1:9])
                    _ = other_mimi.decode(tokens[:, 1:9])
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        print("[INFO] é¢„çƒ­å®Œæˆ")
        
        # 6. åˆå§‹åŒ–ç³»ç»Ÿæç¤º
        print("[INFO] åˆå§‹åŒ–ç³»ç»Ÿæç¤º...")
        text_prompt = "You enjoy having a good conversation."
        from moshi.offline import wrap_with_system_tags
        lm_gen.text_prompt_tokens = (
            text_tokenizer.encode(wrap_with_system_tags(text_prompt)) if len(text_prompt) > 0 else None
        )
        
        # é‡ç½®æµå¼çŠ¶æ€å¹¶è¿è¡Œç³»ç»Ÿæç¤º
        mimi.reset_streaming()
        other_mimi.reset_streaming()
        lm_gen.reset_streaming()
        lm_gen.step_system_prompts(mimi)
        mimi.reset_streaming()
        
        model_status = "å·²åŠ è½½"
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\næ˜¾å­˜ä½¿ç”¨: {mem:.2f} GB"
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}\n\n{traceback.format_exc()}"
        print(error_msg)
        return error_msg

def process_voice(audio):
    """å¤„ç†è¯­éŸ³å¹¶ç”Ÿæˆå›å¤ - ä½¿ç”¨å®˜æ–¹æ–¹å¼"""
    global mimi, other_mimi, lm_gen, text_tokenizer
    
    if mimi is None or lm_gen is None:
        return "è¯·å…ˆåŠ è½½æ¨¡å‹", "âŒ æ¨¡å‹æœªåŠ è½½"
    
    if audio is None:
        return "è¯·è¯´è¯", "âŒ æ²¡æœ‰æ£€æµ‹åˆ°éŸ³é¢‘"
    
    try:
        # 1. é‡ç½®æµå¼çŠ¶æ€ï¼ˆé‡è¦ï¼šç¡®ä¿æ¯æ¬¡å¯¹è¯éƒ½æ˜¯æ–°çš„çŠ¶æ€ï¼‰
        mimi.reset_streaming()
        other_mimi.reset_streaming()
        lm_gen.reset_streaming()
        
        # 2. è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # 3. é‡é‡‡æ ·åˆ°æ¨¡å‹é‡‡æ ·ç‡ (24kHz)
        if sr != mimi.sample_rate:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=mimi.sample_rate)
        
        # 4. è½¬æ¢ä¸º (C, T) æ ¼å¼
        if len(audio_data.shape) == 1:
            audio_data = audio_data[np.newaxis, :]  # (1, T)
        
        # 5. ä½¿ç”¨å®˜æ–¹æ–¹å¼ç¼–ç å’Œå¤„ç†
        user_audio = torch.tensor(audio_data, dtype=torch.float32, device=device)
        generated_frames = []
        generated_text = []
        frame_size = int(mimi.sample_rate / mimi.frame_rate)
        
        # æŒ‰ç…§å®˜æ–¹æ–¹å¼ï¼šå¤„ç†ç”¨æˆ·è¾“å…¥çš„åŒæ—¶ç”Ÿæˆå›å¤
        # æ¨¡å‹ä¼šåœ¨å¤„ç†ç”¨æˆ·è¾“å…¥çš„è¿‡ç¨‹ä¸­å¼€å§‹ç”Ÿæˆå›å¤
        for user_encoded in encode_from_sphn(
            mimi,
            _iterate_audio(user_audio.cpu().numpy(), sample_interval_size=frame_size, pad=True),
            max_batch=1,
        ):
            steps = user_encoded.shape[-1]
            for c in range(steps):
                step_in = user_encoded[:, :, c : c + 1]  # [1, 8, 1]
                tokens = lm_gen.step(input_tokens=step_in)
                
                if tokens is None:
                    continue
                
                # è§£ç éŸ³é¢‘
                pcm = mimi.decode(tokens[:, 1:9])
                _ = other_mimi.decode(tokens[:, 1:9])
                pcm = pcm.detach().cpu().numpy()[0, 0]
                generated_frames.append(pcm)
                
                # è§£ç æ–‡æœ¬
                text_token = tokens[0, 0, 0].item()
                if text_token not in (0, 3):
                    _text = text_tokenizer.id_to_piece(text_token)
                    _text = _text.replace("â–", " ")
                    generated_text.append(_text)
        
        # ç»§ç»­ç”Ÿæˆå›å¤ï¼ˆç”¨æˆ·è¾“å…¥å¤„ç†å®Œåï¼Œç»§ç»­ç”Ÿæˆç›´åˆ°æœ‰è¶³å¤Ÿçš„å›å¤ï¼‰
        # ä½¿ç”¨é™éŸ³è¾“å…¥ç»§ç»­ç”Ÿæˆï¼Œç›´åˆ°æ¨¡å‹åœæ­¢ç”Ÿæˆæˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦
        silence_count = 0
        max_silence = 30  # æœ€å¤šå…è®¸30å¸§é™éŸ³ååœæ­¢
        max_additional_frames = 150  # æœ€å¤šé¢å¤–ç”Ÿæˆ150å¸§
        
        for _ in range(max_additional_frames):
            # ä½¿ç”¨é™éŸ³è¾“å…¥ç»§ç»­ç”Ÿæˆ
            silent_input = torch.zeros(1, 8, 1, dtype=torch.float32, device=device)
            tokens = lm_gen.step(input_tokens=silent_input)
            
            if tokens is None:
                silence_count += 1
                if silence_count > max_silence:
                    break
                continue
            
            # è§£ç éŸ³é¢‘
            pcm = mimi.decode(tokens[:, 1:9])
            _ = other_mimi.decode(tokens[:, 1:9])
            pcm = pcm.detach().cpu().numpy()[0, 0]
            generated_frames.append(pcm)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é™éŸ³
            pcm_abs = np.abs(pcm)
            if np.max(pcm_abs) < 1e-6:
                silence_count += 1
                if silence_count > max_silence:
                    break
            else:
                silence_count = 0  # æœ‰éŸ³é¢‘å†…å®¹ï¼Œé‡ç½®é™éŸ³è®¡æ•°
            
            # è§£ç æ–‡æœ¬
            text_token = tokens[0, 0, 0].item()
            if text_token not in (0, 3):
                _text = text_tokenizer.id_to_piece(text_token)
                _text = _text.replace("â–", " ")
                generated_text.append(_text)
        
        if len(generated_frames) == 0:
            return "æœªç”ŸæˆéŸ³é¢‘", "âŒ æœªç”Ÿæˆä»»ä½•éŸ³é¢‘å¸§"
        
        # ç§»é™¤å‰é¢çš„é™éŸ³å¸§
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰å®é™…éŸ³é¢‘å†…å®¹çš„å¸§
        start_idx = 0
        for i, frame in enumerate(generated_frames):
            if np.max(np.abs(frame)) > 1e-6:
                start_idx = i
                break
        
        if start_idx > 0:
            generated_frames = generated_frames[start_idx:]
            print(f"[INFO] ç§»é™¤äº†å‰ {start_idx} å¸§é™éŸ³")
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        output_audio = np.concatenate(generated_frames, axis=-1)
        output_path = "/tmp/personaplex_output.wav"
        sf.write(output_path, output_audio, mimi.sample_rate)
        
        # ç”Ÿæˆæ–‡æœ¬è¾“å‡º
        user_text = "ğŸ¤ æ‚¨è¯´äº†ï¼š" + (" ".join(generated_text) if generated_text else "ï¼ˆè¯­éŸ³è¾“å…¥ï¼‰")
        ai_text = "ğŸ¤– AI å›å¤ï¼š" + (" ".join(generated_text) if generated_text else "ï¼ˆå¤„ç†ä¸­...ï¼‰")
        
        return user_text, ai_text
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        print(f"[ERROR] {error_msg}\n{traceback.format_exc()}")
        return "å¤„ç†å¤±è´¥", error_msg

# æç®€ç•Œé¢ - åªæœ‰ä¸€ä¸ªè¯´è¯æŒ‰é’®
with gr.Blocks(title="PersonaPlex è¯­éŸ³å¯¹è¯") as demo:
    gr.Markdown("# ğŸ¤ PersonaPlex è¯­éŸ³å¯¹è¯")
    gr.Markdown("ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å¼€å§‹è¯´è¯")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### æ‚¨è¯´çš„è¯")
            user_text = gr.Textbox(label="", value="ç­‰å¾…æ‚¨è¯´è¯...", interactive=False, lines=8)
        
        with gr.Column(scale=1):
            gr.Markdown("### AI å›å¤")
            ai_text = gr.Textbox(label="", value="ç­‰å¾… AI å›å¤...", interactive=False, lines=8)
    
    # åªæœ‰ä¸€ä¸ªéŸ³é¢‘è¾“å…¥ç»„ä»¶ï¼ˆè‡ªåŠ¨å½•éŸ³ï¼‰
    audio_input = gr.Audio(
        label="", 
        type="filepath", 
        sources=["microphone"],
        show_label=False
    )
    
    status = gr.Textbox(label="çŠ¶æ€", value="æ­£åœ¨åŠ è½½æ¨¡å‹...", interactive=False, visible=False)
    
    # è‡ªåŠ¨åŠ è½½æ¨¡å‹
    demo.load(load_model, outputs=status)
    
    # éŸ³é¢‘è¾“å…¥å˜åŒ–æ—¶è‡ªåŠ¨å¤„ç†
    audio_input.change(
        process_voice,
        inputs=[audio_input],
        outputs=[user_text, ai_text]
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=5001)
