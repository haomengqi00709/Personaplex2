#!/usr/bin/env python3
"""
PersonaPlex ç®€å•è¯­éŸ³å¯¹è¯
ä¸€ä¸ªæŒ‰é’®ï¼šå¼€å§‹/åœæ­¢è¯´è¯
å·¦è¾¹ï¼šæ‚¨è¯´çš„è¯ | å³è¾¹ï¼šAIå›å¤
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import MoshiForConditionalGeneration, AutoModel
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
device = "cuda" if torch.cuda.is_available() else "cpu"
model_loading = False
model_status = "æœªåŠ è½½"

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global model, model_status
    
    if model is not None:
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        return f"âœ… æ¨¡å‹å·²åŠ è½½ ({mem:.2f} GB)"
    
    try:
        print("="*60)
        print("å¼€å§‹åŠ è½½æ¨¡å‹")
        print("="*60)
        
        # æ£€æŸ¥ç¯å¢ƒ
        print("\n[DEBUG] æ£€æŸ¥ç¯å¢ƒ...")
        print(f"[DEBUG] MODEL_ID: {MODEL_ID}")
        print(f"[DEBUG] HF_TOKEN: {'å·²è®¾ç½®' if HF_TOKEN else 'æœªè®¾ç½®'}")
        print(f"[DEBUG] Device: {device}")
        
        if HF_TOKEN:
            print("[DEBUG] ç™»å½• Hugging Face...")
            login(token=HF_TOKEN)
            print("[DEBUG] ç™»å½•æˆåŠŸ")
        else:
            print("[DEBUG] âš ï¸  HF_TOKEN æœªè®¾ç½®ï¼Œå¯èƒ½æ— æ³•è®¿é—® gated repo")
        
        model_status = "åŠ è½½ä¸­..."
        
        # æ£€æŸ¥ Transformers ç‰ˆæœ¬å’Œé…ç½®
        import transformers
        from transformers import AutoConfig
        transformers_version = transformers.__version__
        print(f"\n[DEBUG] Transformers ç‰ˆæœ¬: {transformers_version}")
        print(f"[DEBUG] Transformers è·¯å¾„: {transformers.__file__}")
        
        # å°è¯•åŠ è½½é…ç½®ï¼ˆç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼Œä¸é€šè¿‡ AutoConfigï¼‰
        print("\n[DEBUG] æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹é…ç½®å’Œè‡ªå®šä¹‰ä»£ç ...")
        try:
            from huggingface_hub import hf_hub_download
            import json
            
            # ç›´æ¥ä¸‹è½½ config.json
            print("[DEBUG] ç›´æ¥ä¸‹è½½ config.json...")
            config_path = hf_hub_download(
                repo_id=MODEL_ID,
                filename="config.json",
                token=HF_TOKEN
            )
            
            with open(config_path, 'r') as f:
                config_data = json.load(f)
            
            print(f"[DEBUG] âœ… é…ç½®æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
            print(f"[DEBUG] - Model type: {config_data.get('model_type', 'N/A')}")
            print(f"[DEBUG] - Architectures: {config_data.get('architectures', 'N/A')}")
            print(f"[DEBUG] - Auto map: {config_data.get('auto_map', 'N/A')}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç 
            auto_map = config_data.get('auto_map', {})
            if auto_map:
                print(f"[DEBUG] âœ… å‘ç°è‡ªå®šä¹‰ä»£ç æ˜ å°„: {auto_map}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ modeling æ–‡ä»¶
                if 'AutoModel' in auto_map or 'AutoModelForConditionalGeneration' in auto_map:
                    model_file = auto_map.get('AutoModel') or auto_map.get('AutoModelForConditionalGeneration')
                    print(f"[DEBUG] è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶: {model_file}")
                    
                    # å°è¯•ä¸‹è½½è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                    try:
                        custom_code_path = hf_hub_download(
                            repo_id=MODEL_ID,
                            filename=model_file,
                            token=HF_TOKEN
                        )
                        print(f"[DEBUG] âœ… è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {custom_code_path}")
                    except Exception as e:
                        print(f"[DEBUG] âš ï¸  è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            else:
                print("[DEBUG] âš ï¸  æœªæ‰¾åˆ° auto_mapï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†")
                
        except Exception as e:
            print(f"[DEBUG] âš ï¸  é…ç½®æ£€æŸ¥å¤±è´¥ï¼ˆç»§ç»­å°è¯•åŠ è½½ï¼‰: {e}")
            import traceback
            traceback.print_exc()
        
        # å°è¯•å¤šç§åŠ è½½æ–¹å¼
        print("\n[DEBUG] æ­¥éª¤2: å°è¯•åŠ è½½æ¨¡å‹...")
        
        # æ–¹æ³•1: ä½¿ç”¨ AutoModel + trust_remote_codeï¼ˆç»•è¿‡é…ç½®æ£€æŸ¥ï¼‰
        print("[DEBUG] æ–¹æ³•1: ä½¿ç”¨ AutoModel.from_pretrained + trust_remote_code=True")
        print("[DEBUG] æ³¨æ„: å³ä½¿é…ç½®åŠ è½½å¤±è´¥ï¼Œä¹Ÿå°è¯•ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆtrust_remote_code åº”è¯¥ä¼šå¤„ç†è‡ªå®šä¹‰ä»£ç ï¼‰")
        try:
            # ç›´æ¥å°è¯•åŠ è½½ï¼Œè®© trust_remote_code å¤„ç†è‡ªå®šä¹‰ä»£ç 
            model = AutoModel.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True,  # å…³é”®ï¼šè¿™ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
                local_files_only=False  # ç¡®ä¿ä»è¿œç¨‹ä¸‹è½½è‡ªå®šä¹‰ä»£ç 
            )
            print("[DEBUG] âœ… æ–¹æ³•1æˆåŠŸ: AutoModel åŠ è½½æˆåŠŸ")
        except Exception as e1:
            print(f"[DEBUG] âŒ æ–¹æ³•1å¤±è´¥: {type(e1).__name__}: {e1}")
            import traceback
            traceback.print_exc()
            
            # æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç 
            print("\n[DEBUG] æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç ...")
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                from huggingface_hub import list_repo_files
                
                print("[DEBUG] åˆ—å‡ºæ¨¡å‹ä»“åº“æ–‡ä»¶...")
                repo_files = list_repo_files(
                    repo_id=MODEL_ID,
                    token=HF_TOKEN
                )
                print(f"[DEBUG] ä»“åº“æ–‡ä»¶: {[f for f in repo_files if '.py' in f]}")
                
                # æŸ¥æ‰¾ modeling æ–‡ä»¶
                modeling_files = [f for f in repo_files if 'modeling' in f.lower() and f.endswith('.py')]
                if modeling_files:
                    print(f"[DEBUG] æ‰¾åˆ°å»ºæ¨¡æ–‡ä»¶: {modeling_files}")
                    # å°è¯•æ‰‹åŠ¨ä¸‹è½½å¹¶å¯¼å…¥
                    for model_file in modeling_files:
                        try:
                            print(f"[DEBUG] å°è¯•ä¸‹è½½å¹¶å¯¼å…¥: {model_file}")
                            custom_path = hf_hub_download(
                                repo_id=MODEL_ID,
                                filename=model_file,
                                token=HF_TOKEN
                            )
                            print(f"[DEBUG] è‡ªå®šä¹‰ä»£ç è·¯å¾„: {custom_path}")
                            # è¿™é‡Œå¯ä»¥å°è¯•åŠ¨æ€å¯¼å…¥ï¼Œä½†æ¯”è¾ƒå¤æ‚
                        except Exception as e:
                            print(f"[DEBUG] ä¸‹è½½ {model_file} å¤±è´¥: {e}")
                
            except Exception as e2:
                print(f"[DEBUG] âš ï¸  æ–¹æ³•2å¤±è´¥: {e2}")
            
            # æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGenerationï¼ˆä½œä¸ºå›é€€ï¼‰
            print("\n[DEBUG] æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGeneration...")
            try:
                from transformers import MoshiForConditionalGeneration
                model = MoshiForConditionalGeneration.from_pretrained(
                    MODEL_ID,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                print("[DEBUG] âœ… æ–¹æ³•3æˆåŠŸ: MoshiForConditionalGeneration åŠ è½½æˆåŠŸ")
            except Exception as e3:
                print(f"[DEBUG] âŒ æ–¹æ³•3å¤±è´¥: {type(e3).__name__}: {e3}")
                import traceback
                traceback.print_exc()
                
                # æœ€ç»ˆé”™è¯¯å¤„ç†
                error_msg = str(e1)
                return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

å½“å‰ Transformers ç‰ˆæœ¬: {transformers_version}
ä¸»è¦é”™è¯¯: {error_msg}

å·²å°è¯•çš„æ–¹æ³•:
1. AutoModel.from_pretrained + trust_remote_code=True
2. æ‰‹åŠ¨æ£€æŸ¥è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
3. MoshiForConditionalGeneration

é—®é¢˜åˆ†æ:
PersonaPlex ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„ï¼Œéœ€è¦ä»æ¨¡å‹ä»“åº“åŠ è½½è‡ªå®šä¹‰ä»£ç ã€‚
ä½† Transformers åœ¨åŠ è½½é…ç½®æ—¶å°±å¤±è´¥äº†ï¼Œæ— æ³•ç»§ç»­ã€‚

è§£å†³æ–¹æ¡ˆ:
ç”±äº PersonaPlex æ¶æ„å¤ªæ–°ï¼Œå½“å‰ Transformers ç‰ˆæœ¬å¯èƒ½è¿˜ä¸å®Œå…¨æ”¯æŒã€‚
å»ºè®®:
1. ç­‰å¾… Transformers æ›´æ–°æ”¯æŒ PersonaPlex
2. æˆ–ä½¿ç”¨å®˜æ–¹ PersonaPlex ä»£ç åº“: https://github.com/NVIDIA/personaplex
3. æˆ–æ‰‹åŠ¨å®ç°æ¨¡å‹åŠ è½½é€»è¾‘"""
                
                raise e1
        
        # éªŒè¯æ¨¡å‹
        print("\n[DEBUG] æ­¥éª¤3: éªŒè¯æ¨¡å‹...")
        if model is None:
            raise Exception("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel ä¸º None")
        
        print(f"[DEBUG] æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"[DEBUG] æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
        
        model.eval()
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        model_status = "å·²åŠ è½½"
        
        print(f"\n[DEBUG] âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æ˜¾å­˜: {mem:.2f} GB")
        print("="*60)
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼({mem:.2f} GB)"
        
    except Exception as e:
        model_status = "åŠ è½½å¤±è´¥"
        error_msg = str(e)
        error_type = type(e).__name__
        
        print(f"\n[DEBUG] âŒ æœ€ç»ˆé”™è¯¯: {error_type}: {error_msg}")
        import traceback
        traceback.print_exc()
        
        return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯ä¿¡æ¯: {error_msg}

è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚"""

def encode_audio_manual(audio_data, sample_rate=24000):
    """æ‰‹åŠ¨ç¼–ç éŸ³é¢‘ï¼ˆä¸ä¾èµ– processorï¼‰"""
    try:
        print("[DEBUG] å¼€å§‹æ‰‹åŠ¨ç¼–ç éŸ³é¢‘...")
        
        # 1. ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # 2. å½’ä¸€åŒ–åˆ° [-1, 1]
        max_val = np.abs(audio_data).max()
        if max_val > 1.0 or max_val == 0:
            if max_val > 0:
                audio_data = audio_data / max_val
            else:
                audio_data = audio_data.astype(np.float32)
        else:
            audio_data = audio_data.astype(np.float32)
        
        # 3. è½¬æ¢ä¸º tensor
        audio_tensor = torch.from_numpy(audio_data).float()
        
        print(f"[DEBUG] éŸ³é¢‘ç¼–ç å®Œæˆ: shape={audio_tensor.shape}, dtype={audio_tensor.dtype}")
        
        return audio_tensor
        
    except Exception as e:
        print(f"[DEBUG] éŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def process_voice(audio, text_prompt=None):
    """å¤„ç†è¯­éŸ³å¹¶ç”Ÿæˆå›å¤"""
    global model
    
    if model is None:
        return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "âŒ æ¨¡å‹æœªåŠ è½½"
    
    if audio is None:
        return "", ""
    
    # å¤„ç†éŸ³é¢‘è·¯å¾„ï¼ˆGradio å¯èƒ½ä¼ é€’å­—å…¸ï¼‰
    if isinstance(audio, dict):
        audio_path = audio.get('path', audio.get('url', None))
        if audio_path is None:
            return "", ""
    else:
        audio_path = audio
    
    # è®¾ç½®é»˜è®¤æ–‡æœ¬æç¤º
    if text_prompt is None or text_prompt.strip() == "":
        text_prompt = "You are a helpful AI assistant. Respond naturally."
    
    try:
        print("\n[DEBUG] å¼€å§‹å¤„ç†è¯­éŸ³...")
        print(f"[DEBUG] éŸ³é¢‘è·¯å¾„: {audio_path}")
        print(f"[DEBUG] æ–‡æœ¬æç¤º: {text_prompt}")
        
        # è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio_path)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != 24000:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=24000)
            sr = 24000
        
        duration = len(audio_data) / sr
        user_text = f"ğŸ¤ è¯­éŸ³è¾“å…¥ ({duration:.2f}ç§’, {len(audio_data)} é‡‡æ ·ç‚¹)"
        print(f"[DEBUG] å¤„ç†åéŸ³é¢‘: {len(audio_data)} é‡‡æ ·ç‚¹, {sr}Hz, {duration:.2f}ç§’")
        
        # å°è¯•è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
        try:
            # ç¼–ç éŸ³é¢‘
            print("[DEBUG] ç¼–ç éŸ³é¢‘...")
            audio_tensor = encode_audio_manual(audio_data, sr)
            
            if audio_tensor is None:
                raise Exception("éŸ³é¢‘ç¼–ç å¤±è´¥")
            
            print(f"[DEBUG] æ–‡æœ¬æç¤º: {text_prompt}")
            print(f"[DEBUG] éŸ³é¢‘ tensor å½¢çŠ¶: {audio_tensor.shape}")
            
            # å°è¯•è°ƒç”¨æ¨¡å‹
            print("[DEBUG] è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†...")
            with torch.no_grad():
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ generate æ–¹æ³•
                if hasattr(model, 'generate'):
                    print("[DEBUG] ä½¿ç”¨ generate æ–¹æ³•...")
                    try:
                        # å‡†å¤‡éŸ³é¢‘è¾“å…¥ - éœ€è¦ 3D tensor (batch, channels, length) ä¸”ä¸º float16
                        print("[DEBUG] å‡†å¤‡éŸ³é¢‘è¾“å…¥...")
                        print(f"[DEBUG] åŸå§‹éŸ³é¢‘ tensor å½¢çŠ¶: {audio_tensor.shape}, dtype: {audio_tensor.dtype}")
                        
                        # audio_encoder éœ€è¦ 3D: (batch, channels, length) ä¸”ä¸º float16
                        # å¦‚æœ audio_tensor æ˜¯ 1Dï¼Œéœ€è¦æ·»åŠ  batch å’Œ channel ç»´åº¦
                        if len(audio_tensor.shape) == 1:
                            # (length) -> (1, 1, length)
                            audio_input = audio_tensor.unsqueeze(0).unsqueeze(0).to(device)
                        elif len(audio_tensor.shape) == 2:
                            # (batch, length) -> (batch, 1, length)
                            audio_input = audio_tensor.unsqueeze(1).to(device)
                        else:
                            audio_input = audio_tensor.to(device)
                        
                        # è½¬æ¢ä¸º float16ï¼ˆæ¨¡å‹ä½¿ç”¨ float16ï¼‰
                        audio_input = audio_input.half()
                        
                        print(f"[DEBUG] å‡†å¤‡åçš„éŸ³é¢‘è¾“å…¥å½¢çŠ¶: {audio_input.shape}, dtype: {audio_input.dtype}")
                        
                        # æ£€æŸ¥æ¨¡å‹çš„å®é™…ç»“æ„
                        print("[DEBUG] æ£€æŸ¥æ¨¡å‹ç»“æ„...")
                        print(f"[DEBUG] æ¨¡å‹ç±»å‹: {type(model).__name__}")
                        
                        # å°è¯•æŸ¥çœ‹æ¨¡å‹çš„ forward æ–¹æ³•
                        import inspect
                        if hasattr(model, 'forward'):
                            sig = inspect.signature(model.forward)
                            print(f"[DEBUG] Forward ç­¾å: {sig}")
                            print(f"[DEBUG] Forward å‚æ•°: {list(sig.parameters.keys())}")
                        
                        # å°è¯•ç›´æ¥è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
                        try:
                            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰éŸ³é¢‘ç¼–ç å™¨
                            if hasattr(model, 'audio_encoder'):
                                print("[DEBUG] å‘ç° audio_encoderï¼Œå°è¯•ç¼–ç éŸ³é¢‘...")
                                try:
                                    # ä½¿ç”¨ audio_encoder ç¼–ç éŸ³é¢‘
                                    # audio_encoder.encode éœ€è¦ 3D tensor (batch, channels, length)
                                    encoded_result = model.audio_encoder.encode(audio_input)
                                    
                                    # å¤„ç†ç¼–ç ç»“æœ - å¯èƒ½æ˜¯ MimiEncoderOutput å¯¹è±¡
                                    print(f"[DEBUG] ç¼–ç ç»“æœç±»å‹: {type(encoded_result)}")
                                    
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯ MimiEncoderOutput å¯¹è±¡
                                    if hasattr(encoded_result, 'audio_codes'):
                                        # ä» MimiEncoderOutput ä¸­æå– audio_codes
                                        user_audio_codes = encoded_result.audio_codes
                                        print(f"[DEBUG] ä» MimiEncoderOutput æå– audio_codes: shape={user_audio_codes.shape if hasattr(user_audio_codes, 'shape') else 'N/A'}")
                                    elif isinstance(encoded_result, tuple):
                                        print(f"[DEBUG] ç¼–ç ç»“æœ: å…ƒç»„ï¼Œé•¿åº¦={len(encoded_result)}")
                                        for i, item in enumerate(encoded_result):
                                            if hasattr(item, 'shape'):
                                                print(f"[DEBUG] ç¼–ç ç»“æœ[{i}]: shape={item.shape}, dtype={item.dtype}")
                                            elif hasattr(item, 'audio_codes'):
                                                print(f"[DEBUG] ç¼–ç ç»“æœ[{i}]: MimiEncoderOutput with audio_codes")
                                        # å°è¯•ä»å…ƒç»„ä¸­æå–
                                        for item in encoded_result:
                                            if hasattr(item, 'audio_codes'):
                                                user_audio_codes = item.audio_codes
                                                break
                                            elif isinstance(item, torch.Tensor):
                                                user_audio_codes = item
                                                break
                                    elif isinstance(encoded_result, torch.Tensor):
                                        user_audio_codes = encoded_result
                                    else:
                                        # å°è¯•è®¿é—®å¯èƒ½çš„å±æ€§
                                        if hasattr(encoded_result, 'codes'):
                                            user_audio_codes = encoded_result.codes
                                        else:
                                            print(f"[DEBUG] æ— æ³•æå– codesï¼Œä½¿ç”¨åŸå§‹ç»“æœ")
                                            user_audio_codes = encoded_result
                                    
                                    print(f"[DEBUG] æœ€ç»ˆ user_audio_codes: shape={user_audio_codes.shape if hasattr(user_audio_codes, 'shape') else type(user_audio_codes)}")
                                    
                                except Exception as encode_error:
                                    print(f"[DEBUG] audio_encoder è°ƒç”¨å¤±è´¥: {encode_error}")
                                    import traceback
                                    traceback.print_exc()
                                    # å¦‚æœ audio_encoder å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘ï¼ˆå·²ç»æ˜¯ float16ï¼‰
                                    user_audio_codes = None
                                    user_input_values = audio_input
                                    print("[DEBUG] å›é€€åˆ°ä½¿ç”¨ user_input_values (float16)")
                            else:
                                # æ²¡æœ‰ audio_encoderï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘å€¼
                                user_input_values = audio_input
                                user_audio_codes = None
                                print("[DEBUG] æ²¡æœ‰ audio_encoderï¼Œä½¿ç”¨ user_input_values")
                            
                            # å‡†å¤‡æ–‡æœ¬è¾“å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            # å¯¹äº PersonaPlexï¼Œå¯èƒ½éœ€è¦å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸º input_ids
                            input_ids = None
                            if text_prompt:
                                print(f"[DEBUG] å¤„ç†æ–‡æœ¬æç¤º: {text_prompt}")
                                # å°è¯•ä½¿ç”¨ tokenizerï¼ˆå¦‚æœæ¨¡å‹æœ‰ï¼‰
                                if hasattr(model, 'tokenizer') and model.tokenizer is not None:
                                    try:
                                        input_ids = model.tokenizer(text_prompt, return_tensors="pt").input_ids.to(device)
                                        print(f"[DEBUG] æ–‡æœ¬ input_ids: shape={input_ids.shape}")
                                    except:
                                        print("[DEBUG] tokenizer ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æœ¬è¾“å…¥")
                            
                            # è·å– Moshi æ— æ¡ä»¶è¾“å…¥ï¼ˆå¿…éœ€çš„ï¼‰
                            print("[DEBUG] è·å– Moshi æ— æ¡ä»¶è¾“å…¥...")
                            moshi_inputs = {}
                            try:
                                if hasattr(model, 'get_unconditional_inputs'):
                                    # æ£€æŸ¥æ–¹æ³•ç­¾å
                                    import inspect
                                    sig = inspect.signature(model.get_unconditional_inputs)
                                    params = list(sig.parameters.keys())
                                    print(f"[DEBUG] get_unconditional_inputs å‚æ•°: {params}")
                                    
                                    # è°ƒç”¨ get_unconditional_inputsï¼ˆéœ€è¦ num_samples å‚æ•°ï¼‰
                                    moshi_inputs = model.get_unconditional_inputs(num_samples=1)
                                    print(f"[DEBUG] get_unconditional_inputs æˆåŠŸ: {list(moshi_inputs.keys())}")
                                    for key, value in moshi_inputs.items():
                                        if hasattr(value, 'shape'):
                                            print(f"[DEBUG]   {key}: shape={value.shape}, dtype={value.dtype}")
                                else:
                                    print("[DEBUG] æ¨¡å‹æ²¡æœ‰ get_unconditional_inputs æ–¹æ³•")
                                    # æ‰‹åŠ¨åˆ›å»º
                                    if user_audio_codes is not None:
                                        moshi_shape = user_audio_codes.shape
                                        moshi_inputs = {
                                            'moshi_audio_codes': torch.zeros(moshi_shape, dtype=user_audio_codes.dtype, device=device)
                                        }
                                        print(f"[DEBUG] æ‰‹åŠ¨åˆ›å»º moshi_audio_codes, shape: {moshi_shape}")
                            except Exception as e:
                                print(f"[DEBUG] è·å– Moshi è¾“å…¥å¤±è´¥: {e}")
                                import traceback
                                traceback.print_exc()
                                # æœ€åçš„å›é€€ï¼šæ‰‹åŠ¨åˆ›å»º
                                if user_audio_codes is not None:
                                    moshi_shape = user_audio_codes.shape
                                    moshi_inputs = {
                                        'moshi_audio_codes': torch.zeros(moshi_shape, dtype=user_audio_codes.dtype, device=device)
                                    }
                                    print(f"[DEBUG] å›é€€ï¼šæ‰‹åŠ¨åˆ›å»º moshi_audio_codes, shape: {moshi_shape}")
                            
                            # å°è¯•è°ƒç”¨ generate æ–¹æ³•
                            print("[DEBUG] å°è¯•è°ƒç”¨ generate æ–¹æ³•...")
                            try:
                                # æ„å»º generate çš„è¾“å…¥
                                generate_kwargs = {}
                                
                                # ç”¨æˆ·éŸ³é¢‘è¾“å…¥
                                if user_audio_codes is not None:
                                    generate_kwargs['user_audio_codes'] = user_audio_codes
                                    print(f"[DEBUG] user_audio_codes: shape={user_audio_codes.shape}")
                                elif user_input_values is not None:
                                    generate_kwargs['user_input_values'] = user_input_values
                                    print(f"[DEBUG] user_input_values: shape={user_input_values.shape}")
                                
                                # Moshi è¾“å…¥ï¼ˆå¿…éœ€çš„ï¼‰
                                if 'moshi_input_values' in moshi_inputs:
                                    generate_kwargs['moshi_input_values'] = moshi_inputs['moshi_input_values']
                                    print(f"[DEBUG] moshi_input_values: shape={moshi_inputs['moshi_input_values'].shape}")
                                elif 'moshi_audio_codes' in moshi_inputs:
                                    generate_kwargs['moshi_audio_codes'] = moshi_inputs['moshi_audio_codes']
                                    print(f"[DEBUG] moshi_audio_codes: shape={moshi_inputs['moshi_audio_codes'].shape}")
                                
                                # æ–‡æœ¬è¾“å…¥ï¼ˆå¿…éœ€çš„ï¼‰- ä» get_unconditional_inputs è·å–
                                if 'input_ids' in moshi_inputs:
                                    generate_kwargs['input_ids'] = moshi_inputs['input_ids']
                                    print(f"[DEBUG] input_ids: shape={moshi_inputs['input_ids'].shape}")
                                elif input_ids is not None:
                                    generate_kwargs['input_ids'] = input_ids
                                    print(f"[DEBUG] input_ids (from tokenizer): shape={input_ids.shape}")
                                else:
                                    # å¦‚æœæ²¡æœ‰ input_idsï¼Œå°è¯•ä» get_unconditional_inputs è·å–
                                    try:
                                        unconditional = model.get_unconditional_inputs(num_samples=1)
                                        if 'input_ids' in unconditional:
                                            generate_kwargs['input_ids'] = unconditional['input_ids']
                                            print(f"[DEBUG] input_ids (from get_unconditional_inputs): shape={unconditional['input_ids'].shape}")
                                    except:
                                        pass
                                
                                # æ£€æŸ¥å¹¶å¯¹é½åºåˆ—é•¿åº¦ï¼ˆæ‰€æœ‰è¾“å…¥å¿…é¡»é•¿åº¦ä¸€è‡´ï¼‰
                                print("[DEBUG] ========== å¼€å§‹åºåˆ—é•¿åº¦å¯¹é½ ==========")
                                
                                # è·å–æ¯ä¸ªè¾“å…¥çš„åºåˆ—é•¿åº¦ï¼ˆç¬¬äºŒä¸ªç»´åº¦ï¼Œå³ seq_lenï¼‰
                                def get_seq_length(tensor, name):
                                    """è·å– tensor çš„åºåˆ—é•¿åº¦ï¼ˆç¬¬äºŒä¸ªç»´åº¦ï¼‰"""
                                    if tensor is None:
                                        return None
                                    shape = tensor.shape
                                    if len(shape) >= 2:
                                        seq_len = shape[1]  # (batch, seq_len, ...)
                                    else:
                                        seq_len = shape[0]  # (seq_len,)
                                    print(f"[DEBUG] {name}: shape={shape}, seq_len={seq_len}")
                                    return seq_len, shape
                                
                                seq_lengths = {}
                                tensor_shapes = {}
                                
                                if 'input_ids' in generate_kwargs:
                                    seq_len, shape = get_seq_length(generate_kwargs['input_ids'], 'input_ids')
                                    seq_lengths['input_ids'] = seq_len
                                    tensor_shapes['input_ids'] = shape
                                    
                                if 'user_audio_codes' in generate_kwargs:
                                    seq_len, shape = get_seq_length(generate_kwargs['user_audio_codes'], 'user_audio_codes')
                                    seq_lengths['user_audio_codes'] = seq_len
                                    tensor_shapes['user_audio_codes'] = shape
                                    
                                if 'moshi_audio_codes' in generate_kwargs:
                                    seq_len, shape = get_seq_length(generate_kwargs['moshi_audio_codes'], 'moshi_audio_codes')
                                    seq_lengths['moshi_audio_codes'] = seq_len
                                    tensor_shapes['moshi_audio_codes'] = shape
                                
                                print(f"[DEBUG] å½“å‰åºåˆ—é•¿åº¦: {seq_lengths}")
                                print(f"[DEBUG] å½“å‰ tensor å½¢çŠ¶: {tensor_shapes}")
                                
                                # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œéœ€è¦å¯¹é½
                                if len(seq_lengths) > 1:
                                    lengths = [v for v in seq_lengths.values() if v is not None]
                                    if len(set(lengths)) > 1:
                                        print(f"[DEBUG] âš ï¸ åºåˆ—é•¿åº¦ä¸åŒ¹é…ï¼Œå¼€å§‹å¯¹é½...")
                                        
                                        # ç›®æ ‡é•¿åº¦ï¼šä½¿ç”¨ user_audio_codes çš„é•¿åº¦ï¼ˆå› ä¸ºè¿™æ˜¯ç”¨æˆ·è¾“å…¥ï¼‰
                                        target_length = seq_lengths.get('user_audio_codes')
                                        if target_length is None:
                                            # å¦‚æœæ²¡æœ‰ user_audio_codesï¼Œä½¿ç”¨æœ€é•¿çš„
                                            target_length = max(lengths)
                                        
                                        print(f"[DEBUG] ç›®æ ‡åºåˆ—é•¿åº¦: {target_length}")
                                        
                                        # 1. å¯¹é½ input_ids
                                        if 'input_ids' in generate_kwargs:
                                            current_ids = generate_kwargs['input_ids']
                                            current_len = seq_lengths['input_ids']
                                            
                                            if current_len != target_length:
                                                print(f"[DEBUG] [å¯¹é½] input_ids: {current_len} -> {target_length}")
                                                
                                                # è·å– pad_token_idï¼ˆç¡®ä¿ä¸æ˜¯ Noneï¼‰
                                                pad_token_id = getattr(model.config, 'pad_token_id', None)
                                                if pad_token_id is None:
                                                    pad_token_id = getattr(model.config, 'eos_token_id', None)
                                                if pad_token_id is None:
                                                    pad_token_id = 0
                                                
                                                print(f"[DEBUG] ä½¿ç”¨ pad_token_id: {pad_token_id}")
                                                
                                                # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦
                                                pad_length = target_length - current_len
                                                
                                                if pad_length > 0:
                                                    # åˆ›å»ºå¡«å…… tensor
                                                    if len(current_ids.shape) == 2:
                                                        # (batch, seq_len) -> (batch, target_len)
                                                        padding = torch.full(
                                                            (current_ids.shape[0], pad_length),
                                                            pad_token_id,
                                                            dtype=current_ids.dtype,
                                                            device=current_ids.device
                                                        )
                                                        generate_kwargs['input_ids'] = torch.cat([current_ids, padding], dim=1)
                                                        print(f"[DEBUG] [å¯¹é½å®Œæˆ] input_ids: {current_ids.shape} -> {generate_kwargs['input_ids'].shape}")
                                                    else:
                                                        # (seq_len,) -> (target_len,)
                                                        padding = torch.full(
                                                            (pad_length,),
                                                            pad_token_id,
                                                            dtype=current_ids.dtype,
                                                            device=current_ids.device
                                                        )
                                                        generate_kwargs['input_ids'] = torch.cat([current_ids, padding], dim=0)
                                                        print(f"[DEBUG] [å¯¹é½å®Œæˆ] input_ids: {current_ids.shape} -> {generate_kwargs['input_ids'].shape}")
                                        
                                        # 2. å¯¹é½ moshi_audio_codes ä»¥åŒ¹é… user_audio_codes
                                        if 'user_audio_codes' in generate_kwargs and 'moshi_audio_codes' in generate_kwargs:
                                            user_codes = generate_kwargs['user_audio_codes']
                                            moshi_codes = generate_kwargs['moshi_audio_codes']
                                            
                                            user_seq_len = seq_lengths['user_audio_codes']
                                            moshi_seq_len = seq_lengths['moshi_audio_codes']
                                            
                                            print(f"[DEBUG] user_audio_codes: shape={user_codes.shape}, seq_len={user_seq_len}")
                                            print(f"[DEBUG] moshi_audio_codes: shape={moshi_codes.shape}, seq_len={moshi_seq_len}")
                                            
                                            if moshi_seq_len != user_seq_len:
                                                print(f"[DEBUG] [å¯¹é½] moshi_audio_codes: {moshi_seq_len} -> {user_seq_len}")
                                                
                                                # é‡å¤ moshi_codes ä»¥åŒ¹é… user_codes çš„é•¿åº¦
                                                if moshi_seq_len < user_seq_len:
                                                    repeat_times = user_seq_len // moshi_seq_len
                                                    remainder = user_seq_len % moshi_seq_len
                                                    
                                                    print(f"[DEBUG] é‡å¤æ¬¡æ•°: {repeat_times}, ä½™æ•°: {remainder}")
                                                    
                                                    if len(moshi_codes.shape) == 3:
                                                        # (batch, seq_len, code_dim)
                                                        repeated = moshi_codes.repeat(1, repeat_times, 1)
                                                        if remainder > 0:
                                                            repeated = torch.cat([repeated, moshi_codes[:, :remainder, :]], dim=1)
                                                    elif len(moshi_codes.shape) == 2:
                                                        # (batch, seq_len)
                                                        repeated = moshi_codes.repeat(1, repeat_times)
                                                        if remainder > 0:
                                                            repeated = torch.cat([repeated, moshi_codes[:, :remainder]], dim=1)
                                                    else:
                                                        # (seq_len,)
                                                        repeated = moshi_codes.repeat(repeat_times)
                                                        if remainder > 0:
                                                            repeated = torch.cat([repeated, moshi_codes[:remainder]], dim=0)
                                                    
                                                    generate_kwargs['moshi_audio_codes'] = repeated
                                                    print(f"[DEBUG] [å¯¹é½å®Œæˆ] moshi_audio_codes: {moshi_codes.shape} -> {repeated.shape}")
                                        
                                        # éªŒè¯å¯¹é½ç»“æœ
                                        print("[DEBUG] ========== éªŒè¯å¯¹é½ç»“æœ ==========")
                                        final_lengths = {}
                                        final_shapes = {}
                                        
                                        if 'input_ids' in generate_kwargs:
                                            seq_len, shape = get_seq_length(generate_kwargs['input_ids'], 'input_ids (final)')
                                            final_lengths['input_ids'] = seq_len
                                            final_shapes['input_ids'] = shape
                                            
                                        if 'user_audio_codes' in generate_kwargs:
                                            seq_len, shape = get_seq_length(generate_kwargs['user_audio_codes'], 'user_audio_codes (final)')
                                            final_lengths['user_audio_codes'] = seq_len
                                            final_shapes['user_audio_codes'] = shape
                                            
                                        if 'moshi_audio_codes' in generate_kwargs:
                                            seq_len, shape = get_seq_length(generate_kwargs['moshi_audio_codes'], 'moshi_audio_codes (final)')
                                            final_lengths['moshi_audio_codes'] = seq_len
                                            final_shapes['moshi_audio_codes'] = shape
                                        
                                        print(f"[DEBUG] å¯¹é½ååºåˆ—é•¿åº¦: {final_lengths}")
                                        print(f"[DEBUG] å¯¹é½å tensor å½¢çŠ¶: {final_shapes}")
                                        
                                        # æœ€ç»ˆéªŒè¯
                                        final_lengths_list = [v for v in final_lengths.values() if v is not None]
                                        if len(set(final_lengths_list)) > 1:
                                            print(f"[DEBUG] âŒ é”™è¯¯: å¯¹é½åé•¿åº¦ä»ä¸åŒ¹é…!")
                                            print(f"[DEBUG] é•¿åº¦å·®å¼‚: {final_lengths}")
                                            # ä¸ç»§ç»­æ‰§è¡Œï¼Œç›´æ¥è¿”å›é”™è¯¯
                                            ai_text = f"""âŒ åºåˆ—é•¿åº¦å¯¹é½å¤±è´¥

ğŸ“Š å¯¹é½åé•¿åº¦: {final_lengths}
ğŸ“Š å¯¹é½åå½¢çŠ¶: {final_shapes}

âš ï¸ æ— æ³•å¯¹é½è¾“å…¥åºåˆ—é•¿åº¦ï¼Œæ¨¡å‹æ— æ³•å¤„ç†ã€‚
è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"""
                                            return user_text, ai_text
                                        else:
                                            print(f"[DEBUG] âœ… æ‰€æœ‰è¾“å…¥åºåˆ—é•¿åº¦å·²å¯¹é½: {final_lengths_list[0] if final_lengths_list else 'N/A'}")
                                
                                print("[DEBUG] ========== åºåˆ—é•¿åº¦å¯¹é½å®Œæˆ ==========")
                                
                                print(f"[DEBUG] Generate å‚æ•°: {list(generate_kwargs.keys())}")
                                
                                # è°ƒç”¨ generate
                                outputs = model.generate(
                                    **generate_kwargs,
                                    max_new_tokens=128,
                                    temperature=0.7,
                                    do_sample=True
                                )
                                
                                print(f"[DEBUG] Generate æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {outputs.shape if hasattr(outputs, 'shape') else type(outputs)}")
                                
                                # å°è¯•è§£ç è¾“å‡º
                                output_text = "æ¨ç†å®Œæˆ"
                                if hasattr(model, 'tokenizer') and model.tokenizer is not None:
                                    try:
                                        output_text = model.tokenizer.decode(outputs[0], skip_special_tokens=True)
                                        print(f"[DEBUG] è§£ç æ–‡æœ¬: {output_text[:100]}")
                                    except:
                                        pass
                                
                                ai_text = f"""âœ… æ¨ç†æˆåŠŸï¼

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- é‡‡æ ·ç‚¹æ•°: {len(audio_data)}
- æ–‡æœ¬æç¤º: {text_prompt}

ğŸ¤– AI å›å¤:
{output_text}

âœ… æ¨¡å‹æ¨ç†å®Œæˆï¼"""
                                
                            except Exception as gen_error:
                                print(f"[DEBUG] Generate å¤±è´¥: {gen_error}")
                                import traceback
                                traceback.print_exc()
                                
                                # å¦‚æœ generate å¤±è´¥ï¼Œå°è¯• forwardï¼ˆä½¿ç”¨å·²å¯¹é½çš„è¾“å…¥ï¼‰
                                print("[DEBUG] å°è¯•ä½¿ç”¨ forward æ–¹æ³•...")
                                try:
                                    # ä½¿ç”¨å·²ç»å¯¹é½çš„ generate_kwargs
                                    forward_kwargs = generate_kwargs.copy()
                                    
                                    forward_output = model.forward(**forward_kwargs)
                                    print(f"[DEBUG] Forward æˆåŠŸï¼è¾“å‡ºç±»å‹: {type(forward_output)}")
                                    
                                    ai_text = f"""âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸï¼

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}

ğŸ”§ ä½¿ç”¨ forward æ–¹æ³•è°ƒç”¨æˆåŠŸã€‚
è¾“å‡ºç±»å‹: {type(forward_output).__name__}

âš ï¸ æ³¨æ„: éœ€è¦è¿›ä¸€æ­¥å¤„ç†è¾“å‡ºä»¥è·å–æ–‡æœ¬/éŸ³é¢‘å›å¤ã€‚"""
                                    
                                except Exception as forward_error:
                                    print(f"[DEBUG] Forward ä¹Ÿå¤±è´¥: {forward_error}")
                                    import traceback
                                    traceback.print_exc()
                                    ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}

âš ï¸ æ¨¡å‹è°ƒç”¨å¤±è´¥:
- Generate é”™è¯¯: {str(gen_error)[:100]}
- Forward é”™è¯¯: {str(forward_error)[:100]}

è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"""
                            
                        except Exception as forward_error:
                            print(f"[DEBUG] æ•´ä½“è°ƒç”¨å¤±è´¥: {forward_error}")
                            import traceback
                            traceback.print_exc()
                            ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}
- é”™è¯¯: {str(forward_error)}

âš ï¸ éœ€è¦æ ¹æ®æ¨¡å‹æ–‡æ¡£æ„å»ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼ã€‚"""
                        
                    except Exception as e:
                        print(f"[DEBUG] æ¨ç†å°è¯•å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ¨ç†å¤±è´¥: {str(e)}\n\nå¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼ã€‚"
                else:
                    print("[DEBUG] æ¨¡å‹æ²¡æœ‰ generate æ–¹æ³•ï¼Œå°è¯• forward...")
                    # å°è¯•ä½¿ç”¨ forward æ–¹æ³•
                    try:
                        # æ£€æŸ¥æ¨¡å‹çš„ forward ç­¾å
                        import inspect
                        sig = inspect.signature(model.forward)
                        print(f"[DEBUG] Forward æ–¹æ³•ç­¾å: {sig}")
                        print(f"[DEBUG] Forward å‚æ•°: {list(sig.parameters.keys())}")
                        
                        # å°è¯•æŸ¥çœ‹æ¨¡å‹çš„ä¸»è¦ç»„ä»¶
                        print("[DEBUG] æ£€æŸ¥æ¨¡å‹ç»„ä»¶...")
                        model_components = [attr for attr in dir(model) if not attr.startswith('_') and not callable(getattr(model, attr, None))]
                        print(f"[DEBUG] æ¨¡å‹ç»„ä»¶: {model_components[:15]}")
                        
                        ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- é‡‡æ ·ç‚¹æ•°: {len(audio_data)}
- é‡‡æ ·ç‡: {sr}Hz

ğŸ”§ æ¨¡å‹ä¿¡æ¯:
- æ¨¡å‹ç±»å‹: {type(model).__name__}
- Forward å‚æ•°: {list(sig.parameters.keys())}
- æ¨¡å‹ç»„ä»¶: {', '.join(model_components[:10])}

âš ï¸ éœ€è¦æ ¹æ® forward æ–¹æ³•çš„å‚æ•°æ„å»ºæ­£ç¡®çš„è¾“å…¥ã€‚
è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚"""
                    except Exception as e:
                        print(f"[DEBUG] æ£€æŸ¥æ¨¡å‹ç»“æ„å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ— æ³•ç¡®å®šæ¨¡å‹è¾“å…¥æ ¼å¼ã€‚\né”™è¯¯: {str(e)}"
            
        except Exception as e:
            print(f"[DEBUG] æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ¨¡å‹è°ƒç”¨éœ€è¦ç‰¹å®šæ ¼å¼ã€‚\né”™è¯¯: {str(e)}"
        
        return user_text, ai_text
        
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}", ""

# å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
def auto_load_model():
    """ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹"""
    print("="*60)
    print("è‡ªåŠ¨åŠ è½½æ¨¡å‹...")
    print("="*60)
    return load_model()

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="PersonaPlex è¯­éŸ³å¯¹è¯", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ™ï¸ PersonaPlex è¯­éŸ³å¯¹è¯")
    
    # çŠ¶æ€æ˜¾ç¤ºï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
    status = gr.Textbox(
        label="æ¨¡å‹çŠ¶æ€", 
        value="æ­£åœ¨åŠ è½½æ¨¡å‹...", 
        interactive=False,
        lines=3
    )
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
    load_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹", variant="secondary", size="sm")
    
    gr.Markdown("---")
    
    # å¯¹è¯åŒºåŸŸ
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ‘¤ æ‚¨è¯´çš„è¯")
            user_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
        
        with gr.Column():
            gr.Markdown("### ğŸ¤– AI å›å¤")
            ai_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
    
    # æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰
    text_prompt_input = gr.Textbox(
        label="æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰",
        placeholder="ä¾‹å¦‚: You are a helpful AI assistant.",
        lines=2,
        value="You are a helpful AI assistant. Respond naturally."
    )
    
    # è¯­éŸ³è¾“å…¥
    audio_input = gr.Audio(
        label="",
        type="filepath",
        sources=["microphone"],
        format="wav",
        show_label=False
    )
    
    # äº‹ä»¶
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    demo.load(fn=auto_load_model, outputs=status)
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½
    load_btn.click(fn=load_model, outputs=status)
    
    # è¯­éŸ³å¤„ç†ï¼ˆåŒ…å«æ–‡æœ¬æç¤ºï¼‰
    audio_input.change(
        fn=process_voice,
        inputs=[audio_input, text_prompt_input],
        outputs=[user_text, ai_text]
    )
    
    # å¯é€‰ï¼šå¦‚æœæ–‡æœ¬æç¤ºæ”¹å˜ï¼Œä¹Ÿè§¦å‘å¤„ç†ï¼ˆå¦‚æœéŸ³é¢‘å·²å­˜åœ¨ï¼‰
    # text_prompt_input.change(
    #     fn=lambda prompt, audio: process_voice(audio, prompt) if audio else ("", ""),
    #     inputs=[text_prompt_input, audio_input],
    #     outputs=[user_text, ai_text]
    # )

if __name__ == "__main__":
    print("="*60)
    print("PersonaPlex è¯­éŸ³å¯¹è¯ - ç«¯å£ 5001")
    print("="*60)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=5001,
        share=False
    )

