#!/usr/bin/env python3
"""
PersonaPlex ç®€å•è¯­éŸ³å¯¹è¯
ä¸€ä¸ªæŒ‰é’®ï¼šå¼€å§‹/åœæ­¢è¯´è¯
å·¦è¾¹ï¼šæ‚¨è¯´çš„è¯ | å³è¾¹ï¼šAIå›å¤
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import MoshiForConditionalGeneration, AutoModel
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
device = "cuda" if torch.cuda.is_available() else "cpu"
model_loading = False
model_status = "æœªåŠ è½½"

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global model, model_status
    
    if model is not None:
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        return f"âœ… æ¨¡å‹å·²åŠ è½½ ({mem:.2f} GB)"
    
    try:
        print("="*60)
        print("å¼€å§‹åŠ è½½æ¨¡å‹")
        print("="*60)
        
        # æ£€æŸ¥ç¯å¢ƒ
        print("\n[DEBUG] æ£€æŸ¥ç¯å¢ƒ...")
        print(f"[DEBUG] MODEL_ID: {MODEL_ID}")
        print(f"[DEBUG] HF_TOKEN: {'å·²è®¾ç½®' if HF_TOKEN else 'æœªè®¾ç½®'}")
        print(f"[DEBUG] Device: {device}")
        
        if HF_TOKEN:
            print("[DEBUG] ç™»å½• Hugging Face...")
            login(token=HF_TOKEN)
            print("[DEBUG] ç™»å½•æˆåŠŸ")
        else:
            print("[DEBUG] âš ï¸  HF_TOKEN æœªè®¾ç½®ï¼Œå¯èƒ½æ— æ³•è®¿é—® gated repo")
        
        model_status = "åŠ è½½ä¸­..."
        
        # æ£€æŸ¥ Transformers ç‰ˆæœ¬å’Œé…ç½®
        import transformers
        from transformers import AutoConfig
        transformers_version = transformers.__version__
        print(f"\n[DEBUG] Transformers ç‰ˆæœ¬: {transformers_version}")
        print(f"[DEBUG] Transformers è·¯å¾„: {transformers.__file__}")
        
        # å°è¯•åŠ è½½é…ç½®ï¼ˆç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼Œä¸é€šè¿‡ AutoConfigï¼‰
        print("\n[DEBUG] æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹é…ç½®å’Œè‡ªå®šä¹‰ä»£ç ...")
        try:
            from huggingface_hub import hf_hub_download
            import json
            
            # ç›´æ¥ä¸‹è½½ config.json
            print("[DEBUG] ç›´æ¥ä¸‹è½½ config.json...")
            config_path = hf_hub_download(
                repo_id=MODEL_ID,
                filename="config.json",
                token=HF_TOKEN
            )
            
            with open(config_path, 'r') as f:
                config_data = json.load(f)
            
            print(f"[DEBUG] âœ… é…ç½®æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
            print(f"[DEBUG] - Model type: {config_data.get('model_type', 'N/A')}")
            print(f"[DEBUG] - Architectures: {config_data.get('architectures', 'N/A')}")
            print(f"[DEBUG] - Auto map: {config_data.get('auto_map', 'N/A')}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç 
            auto_map = config_data.get('auto_map', {})
            if auto_map:
                print(f"[DEBUG] âœ… å‘ç°è‡ªå®šä¹‰ä»£ç æ˜ å°„: {auto_map}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ modeling æ–‡ä»¶
                if 'AutoModel' in auto_map or 'AutoModelForConditionalGeneration' in auto_map:
                    model_file = auto_map.get('AutoModel') or auto_map.get('AutoModelForConditionalGeneration')
                    print(f"[DEBUG] è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶: {model_file}")
                    
                    # å°è¯•ä¸‹è½½è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                    try:
                        custom_code_path = hf_hub_download(
                            repo_id=MODEL_ID,
                            filename=model_file,
                            token=HF_TOKEN
                        )
                        print(f"[DEBUG] âœ… è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {custom_code_path}")
                    except Exception as e:
                        print(f"[DEBUG] âš ï¸  è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            else:
                print("[DEBUG] âš ï¸  æœªæ‰¾åˆ° auto_mapï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†")
                
        except Exception as e:
            print(f"[DEBUG] âš ï¸  é…ç½®æ£€æŸ¥å¤±è´¥ï¼ˆç»§ç»­å°è¯•åŠ è½½ï¼‰: {e}")
            import traceback
            traceback.print_exc()
        
        # å°è¯•å¤šç§åŠ è½½æ–¹å¼
        print("\n[DEBUG] æ­¥éª¤2: å°è¯•åŠ è½½æ¨¡å‹...")
        
        # æ–¹æ³•1: ä½¿ç”¨ AutoModel + trust_remote_codeï¼ˆç»•è¿‡é…ç½®æ£€æŸ¥ï¼‰
        print("[DEBUG] æ–¹æ³•1: ä½¿ç”¨ AutoModel.from_pretrained + trust_remote_code=True")
        print("[DEBUG] æ³¨æ„: å³ä½¿é…ç½®åŠ è½½å¤±è´¥ï¼Œä¹Ÿå°è¯•ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆtrust_remote_code åº”è¯¥ä¼šå¤„ç†è‡ªå®šä¹‰ä»£ç ï¼‰")
        try:
            # ç›´æ¥å°è¯•åŠ è½½ï¼Œè®© trust_remote_code å¤„ç†è‡ªå®šä¹‰ä»£ç 
            model = AutoModel.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True,  # å…³é”®ï¼šè¿™ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
                local_files_only=False  # ç¡®ä¿ä»è¿œç¨‹ä¸‹è½½è‡ªå®šä¹‰ä»£ç 
            )
            print("[DEBUG] âœ… æ–¹æ³•1æˆåŠŸ: AutoModel åŠ è½½æˆåŠŸ")
        except Exception as e1:
            print(f"[DEBUG] âŒ æ–¹æ³•1å¤±è´¥: {type(e1).__name__}: {e1}")
            import traceback
            traceback.print_exc()
            
            # æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç 
            print("\n[DEBUG] æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç ...")
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                from huggingface_hub import list_repo_files
                
                print("[DEBUG] åˆ—å‡ºæ¨¡å‹ä»“åº“æ–‡ä»¶...")
                repo_files = list_repo_files(
                    repo_id=MODEL_ID,
                    token=HF_TOKEN
                )
                print(f"[DEBUG] ä»“åº“æ–‡ä»¶: {[f for f in repo_files if '.py' in f]}")
                
                # æŸ¥æ‰¾ modeling æ–‡ä»¶
                modeling_files = [f for f in repo_files if 'modeling' in f.lower() and f.endswith('.py')]
                if modeling_files:
                    print(f"[DEBUG] æ‰¾åˆ°å»ºæ¨¡æ–‡ä»¶: {modeling_files}")
                    # å°è¯•æ‰‹åŠ¨ä¸‹è½½å¹¶å¯¼å…¥
                    for model_file in modeling_files:
                        try:
                            print(f"[DEBUG] å°è¯•ä¸‹è½½å¹¶å¯¼å…¥: {model_file}")
                            custom_path = hf_hub_download(
                                repo_id=MODEL_ID,
                                filename=model_file,
                                token=HF_TOKEN
                            )
                            print(f"[DEBUG] è‡ªå®šä¹‰ä»£ç è·¯å¾„: {custom_path}")
                            # è¿™é‡Œå¯ä»¥å°è¯•åŠ¨æ€å¯¼å…¥ï¼Œä½†æ¯”è¾ƒå¤æ‚
                        except Exception as e:
                            print(f"[DEBUG] ä¸‹è½½ {model_file} å¤±è´¥: {e}")
                
            except Exception as e2:
                print(f"[DEBUG] âš ï¸  æ–¹æ³•2å¤±è´¥: {e2}")
            
            # æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGenerationï¼ˆä½œä¸ºå›é€€ï¼‰
            print("\n[DEBUG] æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGeneration...")
            try:
                from transformers import MoshiForConditionalGeneration
                model = MoshiForConditionalGeneration.from_pretrained(
                    MODEL_ID,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                print("[DEBUG] âœ… æ–¹æ³•3æˆåŠŸ: MoshiForConditionalGeneration åŠ è½½æˆåŠŸ")
            except Exception as e3:
                print(f"[DEBUG] âŒ æ–¹æ³•3å¤±è´¥: {type(e3).__name__}: {e3}")
                import traceback
                traceback.print_exc()
                
                # æœ€ç»ˆé”™è¯¯å¤„ç†
                error_msg = str(e1)
                return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

å½“å‰ Transformers ç‰ˆæœ¬: {transformers_version}
ä¸»è¦é”™è¯¯: {error_msg}

å·²å°è¯•çš„æ–¹æ³•:
1. AutoModel.from_pretrained + trust_remote_code=True
2. æ‰‹åŠ¨æ£€æŸ¥è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
3. MoshiForConditionalGeneration

é—®é¢˜åˆ†æ:
PersonaPlex ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„ï¼Œéœ€è¦ä»æ¨¡å‹ä»“åº“åŠ è½½è‡ªå®šä¹‰ä»£ç ã€‚
ä½† Transformers åœ¨åŠ è½½é…ç½®æ—¶å°±å¤±è´¥äº†ï¼Œæ— æ³•ç»§ç»­ã€‚

è§£å†³æ–¹æ¡ˆ:
ç”±äº PersonaPlex æ¶æ„å¤ªæ–°ï¼Œå½“å‰ Transformers ç‰ˆæœ¬å¯èƒ½è¿˜ä¸å®Œå…¨æ”¯æŒã€‚
å»ºè®®:
1. ç­‰å¾… Transformers æ›´æ–°æ”¯æŒ PersonaPlex
2. æˆ–ä½¿ç”¨å®˜æ–¹ PersonaPlex ä»£ç åº“: https://github.com/NVIDIA/personaplex
3. æˆ–æ‰‹åŠ¨å®ç°æ¨¡å‹åŠ è½½é€»è¾‘"""
                
                raise e1
        
        # éªŒè¯æ¨¡å‹
        print("\n[DEBUG] æ­¥éª¤3: éªŒè¯æ¨¡å‹...")
        if model is None:
            raise Exception("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel ä¸º None")
        
        print(f"[DEBUG] æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"[DEBUG] æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
        
        model.eval()
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        model_status = "å·²åŠ è½½"
        
        print(f"\n[DEBUG] âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æ˜¾å­˜: {mem:.2f} GB")
        print("="*60)
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼({mem:.2f} GB)"
        
    except Exception as e:
        model_status = "åŠ è½½å¤±è´¥"
        error_msg = str(e)
        error_type = type(e).__name__
        
        print(f"\n[DEBUG] âŒ æœ€ç»ˆé”™è¯¯: {error_type}: {error_msg}")
        import traceback
        traceback.print_exc()
        
        return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯ä¿¡æ¯: {error_msg}

è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚"""

def encode_audio_manual(audio_data, sample_rate=24000):
    """æ‰‹åŠ¨ç¼–ç éŸ³é¢‘ï¼ˆä¸ä¾èµ– processorï¼‰"""
    try:
        print("[DEBUG] å¼€å§‹æ‰‹åŠ¨ç¼–ç éŸ³é¢‘...")
        
        # 1. ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # 2. å½’ä¸€åŒ–åˆ° [-1, 1]
        max_val = np.abs(audio_data).max()
        if max_val > 1.0 or max_val == 0:
            if max_val > 0:
                audio_data = audio_data / max_val
            else:
                audio_data = audio_data.astype(np.float32)
        else:
            audio_data = audio_data.astype(np.float32)
        
        # 3. è½¬æ¢ä¸º tensor
        audio_tensor = torch.from_numpy(audio_data).float()
        
        print(f"[DEBUG] éŸ³é¢‘ç¼–ç å®Œæˆ: shape={audio_tensor.shape}, dtype={audio_tensor.dtype}")
        
        return audio_tensor
        
    except Exception as e:
        print(f"[DEBUG] éŸ³é¢‘ç¼–ç å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def process_voice(audio, text_prompt=None):
    """å¤„ç†è¯­éŸ³å¹¶ç”Ÿæˆå›å¤"""
    global model
    
    if model is None:
        return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "âŒ æ¨¡å‹æœªåŠ è½½"
    
    if audio is None:
        return "", ""
    
    # å¤„ç†éŸ³é¢‘è·¯å¾„ï¼ˆGradio å¯èƒ½ä¼ é€’å­—å…¸ï¼‰
    if isinstance(audio, dict):
        audio_path = audio.get('path', audio.get('url', None))
        if audio_path is None:
            return "", ""
    else:
        audio_path = audio
    
    # è®¾ç½®é»˜è®¤æ–‡æœ¬æç¤º
    if text_prompt is None or text_prompt.strip() == "":
        text_prompt = "You are a helpful AI assistant. Respond naturally."
    
    try:
        print("\n[DEBUG] å¼€å§‹å¤„ç†è¯­éŸ³...")
        print(f"[DEBUG] éŸ³é¢‘è·¯å¾„: {audio_path}")
        print(f"[DEBUG] æ–‡æœ¬æç¤º: {text_prompt}")
        
        # è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio_path)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != 24000:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=24000)
            sr = 24000
        
        duration = len(audio_data) / sr
        user_text = f"ğŸ¤ è¯­éŸ³è¾“å…¥ ({duration:.2f}ç§’, {len(audio_data)} é‡‡æ ·ç‚¹)"
        print(f"[DEBUG] å¤„ç†åéŸ³é¢‘: {len(audio_data)} é‡‡æ ·ç‚¹, {sr}Hz, {duration:.2f}ç§’")
        
        # å°è¯•è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
        try:
            # ç¼–ç éŸ³é¢‘
            print("[DEBUG] ç¼–ç éŸ³é¢‘...")
            audio_tensor = encode_audio_manual(audio_data, sr)
            
            if audio_tensor is None:
                raise Exception("éŸ³é¢‘ç¼–ç å¤±è´¥")
            
            print(f"[DEBUG] æ–‡æœ¬æç¤º: {text_prompt}")
            print(f"[DEBUG] éŸ³é¢‘ tensor å½¢çŠ¶: {audio_tensor.shape}")
            
            # å°è¯•è°ƒç”¨æ¨¡å‹
            print("[DEBUG] è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†...")
            with torch.no_grad():
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ generate æ–¹æ³•
                if hasattr(model, 'generate'):
                    print("[DEBUG] ä½¿ç”¨ generate æ–¹æ³•...")
                    try:
                        # å°è¯•æ„å»ºè¾“å…¥
                        audio_input = audio_tensor.unsqueeze(0).to(device)
                        
                        # å¦‚æœæ¨¡å‹æœ‰ encode_audio æ–¹æ³•
                        if hasattr(model, 'encode_audio'):
                            print("[DEBUG] ä½¿ç”¨ encode_audio æ–¹æ³•...")
                            encoded_audio = model.encode_audio(audio_input)
                            print(f"[DEBUG] ç¼–ç åå½¢çŠ¶: {encoded_audio.shape if hasattr(encoded_audio, 'shape') else 'N/A'}")
                        
                        # å°è¯•ç”Ÿæˆ
                        print("[DEBUG] å°è¯•ç”Ÿæˆå›å¤...")
                        
                        # æ£€æŸ¥æ¨¡å‹çš„å®é™…ç»“æ„
                        print("[DEBUG] æ£€æŸ¥æ¨¡å‹ç»“æ„...")
                        print(f"[DEBUG] æ¨¡å‹ç±»å‹: {type(model).__name__}")
                        print(f"[DEBUG] æ¨¡å‹å±æ€§: {[attr for attr in dir(model) if not attr.startswith('_')][:20]}")
                        
                        # å°è¯•æŸ¥çœ‹æ¨¡å‹çš„ forward æ–¹æ³•
                        import inspect
                        if hasattr(model, 'forward'):
                            sig = inspect.signature(model.forward)
                            print(f"[DEBUG] Forward ç­¾å: {sig}")
                            print(f"[DEBUG] Forward å‚æ•°: {list(sig.parameters.keys())}")
                        
                        # å°è¯•ç›´æ¥è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
                        try:
                            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰éŸ³é¢‘ç¼–ç å™¨
                            if hasattr(model, 'audio_encoder'):
                                print("[DEBUG] å‘ç° audio_encoderï¼Œå°è¯•ç¼–ç éŸ³é¢‘...")
                                try:
                                    # ä½¿ç”¨ audio_encoder ç¼–ç éŸ³é¢‘
                                    # æ³¨æ„ï¼šaudio_encoder å¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼
                                    encoded_result = model.audio_encoder(audio_input)
                                    
                                    # å¤„ç†ç¼–ç ç»“æœï¼ˆå¯èƒ½æ˜¯å…ƒç»„ï¼‰
                                    if isinstance(encoded_result, tuple):
                                        print(f"[DEBUG] ç¼–ç ç»“æœ: å…ƒç»„ï¼Œé•¿åº¦={len(encoded_result)}")
                                        for i, item in enumerate(encoded_result):
                                            if hasattr(item, 'shape'):
                                                print(f"[DEBUG] ç¼–ç ç»“æœ[{i}]: shape={item.shape}")
                                        # é€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯ç¼–ç åçš„å€¼ï¼Œç¬¬äºŒä¸ªæ˜¯ codes
                                        user_audio_codes = encoded_result[0] if len(encoded_result) > 0 else None
                                        if len(encoded_result) > 1:
                                            user_audio_codes = encoded_result[1]  # codes é€šå¸¸åœ¨ç¬¬äºŒä¸ªä½ç½®
                                    else:
                                        print(f"[DEBUG] ç¼–ç ç»“æœ: shape={encoded_result.shape if hasattr(encoded_result, 'shape') else type(encoded_result)}")
                                        user_audio_codes = encoded_result
                                    
                                    print(f"[DEBUG] ä½¿ç”¨ user_audio_codes: shape={user_audio_codes.shape if hasattr(user_audio_codes, 'shape') else 'N/A'}")
                                    
                                except Exception as encode_error:
                                    print(f"[DEBUG] audio_encoder è°ƒç”¨å¤±è´¥: {encode_error}")
                                    import traceback
                                    traceback.print_exc()
                                    # å¦‚æœ audio_encoder å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘
                                    user_audio_codes = None
                                    user_input_values = audio_input
                                    print("[DEBUG] å›é€€åˆ°ä½¿ç”¨ user_input_values")
                            else:
                                # æ²¡æœ‰ audio_encoderï¼Œç›´æ¥ä½¿ç”¨åŸå§‹éŸ³é¢‘å€¼
                                user_input_values = audio_input
                                user_audio_codes = None
                                print("[DEBUG] æ²¡æœ‰ audio_encoderï¼Œä½¿ç”¨ user_input_values")
                            
                            # å‡†å¤‡æ–‡æœ¬è¾“å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            # å¯¹äº PersonaPlexï¼Œå¯èƒ½éœ€è¦å°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸º input_ids
                            input_ids = None
                            if text_prompt:
                                print(f"[DEBUG] å¤„ç†æ–‡æœ¬æç¤º: {text_prompt}")
                                # å°è¯•ä½¿ç”¨ tokenizerï¼ˆå¦‚æœæ¨¡å‹æœ‰ï¼‰
                                if hasattr(model, 'tokenizer') and model.tokenizer is not None:
                                    try:
                                        input_ids = model.tokenizer(text_prompt, return_tensors="pt").input_ids.to(device)
                                        print(f"[DEBUG] æ–‡æœ¬ input_ids: shape={input_ids.shape}")
                                    except:
                                        print("[DEBUG] tokenizer ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æœ¬è¾“å…¥")
                            
                            # å°è¯•è°ƒç”¨ generate æ–¹æ³•
                            print("[DEBUG] å°è¯•è°ƒç”¨ generate æ–¹æ³•...")
                            try:
                                # æ„å»º generate çš„è¾“å…¥
                                generate_kwargs = {}
                                
                                if user_audio_codes is not None:
                                    generate_kwargs['user_audio_codes'] = user_audio_codes
                                elif user_input_values is not None:
                                    generate_kwargs['user_input_values'] = user_input_values
                                
                                if input_ids is not None:
                                    generate_kwargs['input_ids'] = input_ids
                                
                                print(f"[DEBUG] Generate å‚æ•°: {list(generate_kwargs.keys())}")
                                
                                # è°ƒç”¨ generate
                                outputs = model.generate(
                                    **generate_kwargs,
                                    max_new_tokens=128,
                                    temperature=0.7,
                                    do_sample=True
                                )
                                
                                print(f"[DEBUG] Generate æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {outputs.shape if hasattr(outputs, 'shape') else type(outputs)}")
                                
                                # å°è¯•è§£ç è¾“å‡º
                                output_text = "æ¨ç†å®Œæˆ"
                                if hasattr(model, 'tokenizer') and model.tokenizer is not None:
                                    try:
                                        output_text = model.tokenizer.decode(outputs[0], skip_special_tokens=True)
                                        print(f"[DEBUG] è§£ç æ–‡æœ¬: {output_text[:100]}")
                                    except:
                                        pass
                                
                                ai_text = f"""âœ… æ¨ç†æˆåŠŸï¼

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- é‡‡æ ·ç‚¹æ•°: {len(audio_data)}
- æ–‡æœ¬æç¤º: {text_prompt}

ğŸ¤– AI å›å¤:
{output_text}

âœ… æ¨¡å‹æ¨ç†å®Œæˆï¼"""
                                
                            except Exception as gen_error:
                                print(f"[DEBUG] Generate å¤±è´¥: {gen_error}")
                                import traceback
                                traceback.print_exc()
                                
                                # å¦‚æœ generate å¤±è´¥ï¼Œå°è¯• forward
                                print("[DEBUG] å°è¯•ä½¿ç”¨ forward æ–¹æ³•...")
                                try:
                                    forward_kwargs = {}
                                    if user_audio_codes is not None:
                                        forward_kwargs['user_audio_codes'] = user_audio_codes
                                    elif user_input_values is not None:
                                        forward_kwargs['user_input_values'] = user_input_values
                                    
                                    if input_ids is not None:
                                        forward_kwargs['input_ids'] = input_ids
                                    
                                    forward_output = model.forward(**forward_kwargs)
                                    print(f"[DEBUG] Forward æˆåŠŸï¼è¾“å‡ºç±»å‹: {type(forward_output)}")
                                    
                                    ai_text = f"""âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸï¼

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}

ğŸ”§ ä½¿ç”¨ forward æ–¹æ³•è°ƒç”¨æˆåŠŸã€‚
è¾“å‡ºç±»å‹: {type(forward_output).__name__}

âš ï¸ æ³¨æ„: éœ€è¦è¿›ä¸€æ­¥å¤„ç†è¾“å‡ºä»¥è·å–æ–‡æœ¬/éŸ³é¢‘å›å¤ã€‚"""
                                    
                                except Exception as forward_error:
                                    print(f"[DEBUG] Forward ä¹Ÿå¤±è´¥: {forward_error}")
                                    import traceback
                                    traceback.print_exc()
                                    ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}

âš ï¸ æ¨¡å‹è°ƒç”¨å¤±è´¥:
- Generate é”™è¯¯: {str(gen_error)[:100]}
- Forward é”™è¯¯: {str(forward_error)[:100]}

è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚"""
                            
                        except Exception as forward_error:
                            print(f"[DEBUG] æ•´ä½“è°ƒç”¨å¤±è´¥: {forward_error}")
                            import traceback
                            traceback.print_exc()
                            ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- æ¨¡å‹ç±»å‹: {type(model).__name__}
- é”™è¯¯: {str(forward_error)}

âš ï¸ éœ€è¦æ ¹æ®æ¨¡å‹æ–‡æ¡£æ„å»ºæ­£ç¡®çš„è¾“å…¥æ ¼å¼ã€‚"""
                        
                    except Exception as e:
                        print(f"[DEBUG] æ¨ç†å°è¯•å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ¨ç†å¤±è´¥: {str(e)}\n\nå¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼ã€‚"
                else:
                    print("[DEBUG] æ¨¡å‹æ²¡æœ‰ generate æ–¹æ³•ï¼Œå°è¯• forward...")
                    # å°è¯•ä½¿ç”¨ forward æ–¹æ³•
                    try:
                        # æ£€æŸ¥æ¨¡å‹çš„ forward ç­¾å
                        import inspect
                        sig = inspect.signature(model.forward)
                        print(f"[DEBUG] Forward æ–¹æ³•ç­¾å: {sig}")
                        print(f"[DEBUG] Forward å‚æ•°: {list(sig.parameters.keys())}")
                        
                        # å°è¯•æŸ¥çœ‹æ¨¡å‹çš„ä¸»è¦ç»„ä»¶
                        print("[DEBUG] æ£€æŸ¥æ¨¡å‹ç»„ä»¶...")
                        model_components = [attr for attr in dir(model) if not attr.startswith('_') and not callable(getattr(model, attr, None))]
                        print(f"[DEBUG] æ¨¡å‹ç»„ä»¶: {model_components[:15]}")
                        
                        ai_text = f"""âœ… éŸ³é¢‘å·²å¤„ç†

ğŸ“Š å¤„ç†ä¿¡æ¯:
- éŸ³é¢‘é•¿åº¦: {duration:.2f}ç§’
- é‡‡æ ·ç‚¹æ•°: {len(audio_data)}
- é‡‡æ ·ç‡: {sr}Hz

ğŸ”§ æ¨¡å‹ä¿¡æ¯:
- æ¨¡å‹ç±»å‹: {type(model).__name__}
- Forward å‚æ•°: {list(sig.parameters.keys())}
- æ¨¡å‹ç»„ä»¶: {', '.join(model_components[:10])}

âš ï¸ éœ€è¦æ ¹æ® forward æ–¹æ³•çš„å‚æ•°æ„å»ºæ­£ç¡®çš„è¾“å…¥ã€‚
è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚"""
                    except Exception as e:
                        print(f"[DEBUG] æ£€æŸ¥æ¨¡å‹ç»“æ„å¤±è´¥: {e}")
                        import traceback
                        traceback.print_exc()
                        ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ— æ³•ç¡®å®šæ¨¡å‹è¾“å…¥æ ¼å¼ã€‚\né”™è¯¯: {str(e)}"
            
        except Exception as e:
            print(f"[DEBUG] æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ¨¡å‹è°ƒç”¨éœ€è¦ç‰¹å®šæ ¼å¼ã€‚\né”™è¯¯: {str(e)}"
        
        return user_text, ai_text
        
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}", ""

# å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
def auto_load_model():
    """ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹"""
    print("="*60)
    print("è‡ªåŠ¨åŠ è½½æ¨¡å‹...")
    print("="*60)
    return load_model()

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="PersonaPlex è¯­éŸ³å¯¹è¯", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ™ï¸ PersonaPlex è¯­éŸ³å¯¹è¯")
    
    # çŠ¶æ€æ˜¾ç¤ºï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
    status = gr.Textbox(
        label="æ¨¡å‹çŠ¶æ€", 
        value="æ­£åœ¨åŠ è½½æ¨¡å‹...", 
        interactive=False,
        lines=3
    )
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
    load_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹", variant="secondary", size="sm")
    
    gr.Markdown("---")
    
    # å¯¹è¯åŒºåŸŸ
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ‘¤ æ‚¨è¯´çš„è¯")
            user_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
        
        with gr.Column():
            gr.Markdown("### ğŸ¤– AI å›å¤")
            ai_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
    
    # æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰
    text_prompt_input = gr.Textbox(
        label="æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰",
        placeholder="ä¾‹å¦‚: You are a helpful AI assistant.",
        lines=2,
        value="You are a helpful AI assistant. Respond naturally."
    )
    
    # è¯­éŸ³è¾“å…¥
    audio_input = gr.Audio(
        label="",
        type="filepath",
        sources=["microphone"],
        format="wav",
        show_label=False
    )
    
    # äº‹ä»¶
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    demo.load(fn=auto_load_model, outputs=status)
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½
    load_btn.click(fn=load_model, outputs=status)
    
    # è¯­éŸ³å¤„ç†ï¼ˆåŒ…å«æ–‡æœ¬æç¤ºï¼‰
    audio_input.change(
        fn=process_voice,
        inputs=[audio_input, text_prompt_input],
        outputs=[user_text, ai_text]
    )
    
    # å¯é€‰ï¼šå¦‚æœæ–‡æœ¬æç¤ºæ”¹å˜ï¼Œä¹Ÿè§¦å‘å¤„ç†ï¼ˆå¦‚æœéŸ³é¢‘å·²å­˜åœ¨ï¼‰
    # text_prompt_input.change(
    #     fn=lambda prompt, audio: process_voice(audio, prompt) if audio else ("", ""),
    #     inputs=[text_prompt_input, audio_input],
    #     outputs=[user_text, ai_text]
    # )

if __name__ == "__main__":
    print("="*60)
    print("PersonaPlex è¯­éŸ³å¯¹è¯ - ç«¯å£ 5001")
    print("="*60)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=5001,
        share=False
    )

