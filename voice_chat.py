#!/usr/bin/env python3
"""
PersonaPlex ç®€å•è¯­éŸ³å¯¹è¯
ä¸€ä¸ªæŒ‰é’®ï¼šå¼€å§‹/åœæ­¢è¯´è¯
å·¦è¾¹ï¼šæ‚¨è¯´çš„è¯ | å³è¾¹ï¼šAIå›å¤
"""

import os
import torch
import numpy as np
import soundfile as sf
import gradio as gr
from transformers import MoshiForConditionalGeneration, AutoModel
from huggingface_hub import login
import warnings
warnings.filterwarnings("ignore")

MODEL_ID = "nvidia/personaplex-7b-v1"
HF_TOKEN = os.getenv("HF_TOKEN")
model = None
device = "cuda" if torch.cuda.is_available() else "cpu"
model_loading = False
model_status = "æœªåŠ è½½"

def load_model():
    """åŠ è½½æ¨¡å‹"""
    global model, model_status
    
    if model is not None:
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        return f"âœ… æ¨¡å‹å·²åŠ è½½ ({mem:.2f} GB)"
    
    try:
        print("="*60)
        print("å¼€å§‹åŠ è½½æ¨¡å‹")
        print("="*60)
        
        # æ£€æŸ¥ç¯å¢ƒ
        print("\n[DEBUG] æ£€æŸ¥ç¯å¢ƒ...")
        print(f"[DEBUG] MODEL_ID: {MODEL_ID}")
        print(f"[DEBUG] HF_TOKEN: {'å·²è®¾ç½®' if HF_TOKEN else 'æœªè®¾ç½®'}")
        print(f"[DEBUG] Device: {device}")
        
        if HF_TOKEN:
            print("[DEBUG] ç™»å½• Hugging Face...")
            login(token=HF_TOKEN)
            print("[DEBUG] ç™»å½•æˆåŠŸ")
        else:
            print("[DEBUG] âš ï¸  HF_TOKEN æœªè®¾ç½®ï¼Œå¯èƒ½æ— æ³•è®¿é—® gated repo")
        
        model_status = "åŠ è½½ä¸­..."
        
        # æ£€æŸ¥ Transformers ç‰ˆæœ¬å’Œé…ç½®
        import transformers
        from transformers import AutoConfig
        transformers_version = transformers.__version__
        print(f"\n[DEBUG] Transformers ç‰ˆæœ¬: {transformers_version}")
        print(f"[DEBUG] Transformers è·¯å¾„: {transformers.__file__}")
        
        # å°è¯•åŠ è½½é…ç½®ï¼ˆç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼Œä¸é€šè¿‡ AutoConfigï¼‰
        print("\n[DEBUG] æ­¥éª¤1: æ£€æŸ¥æ¨¡å‹é…ç½®å’Œè‡ªå®šä¹‰ä»£ç ...")
        try:
            from huggingface_hub import hf_hub_download
            import json
            
            # ç›´æ¥ä¸‹è½½ config.json
            print("[DEBUG] ç›´æ¥ä¸‹è½½ config.json...")
            config_path = hf_hub_download(
                repo_id=MODEL_ID,
                filename="config.json",
                token=HF_TOKEN
            )
            
            with open(config_path, 'r') as f:
                config_data = json.load(f)
            
            print(f"[DEBUG] âœ… é…ç½®æ–‡ä»¶ä¸‹è½½æˆåŠŸ")
            print(f"[DEBUG] - Model type: {config_data.get('model_type', 'N/A')}")
            print(f"[DEBUG] - Architectures: {config_data.get('architectures', 'N/A')}")
            print(f"[DEBUG] - Auto map: {config_data.get('auto_map', 'N/A')}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç 
            auto_map = config_data.get('auto_map', {})
            if auto_map:
                print(f"[DEBUG] âœ… å‘ç°è‡ªå®šä¹‰ä»£ç æ˜ å°„: {auto_map}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ modeling æ–‡ä»¶
                if 'AutoModel' in auto_map or 'AutoModelForConditionalGeneration' in auto_map:
                    model_file = auto_map.get('AutoModel') or auto_map.get('AutoModelForConditionalGeneration')
                    print(f"[DEBUG] è‡ªå®šä¹‰æ¨¡å‹æ–‡ä»¶: {model_file}")
                    
                    # å°è¯•ä¸‹è½½è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                    try:
                        custom_code_path = hf_hub_download(
                            repo_id=MODEL_ID,
                            filename=model_file,
                            token=HF_TOKEN
                        )
                        print(f"[DEBUG] âœ… è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {custom_code_path}")
                    except Exception as e:
                        print(f"[DEBUG] âš ï¸  è‡ªå®šä¹‰ä»£ç æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            else:
                print("[DEBUG] âš ï¸  æœªæ‰¾åˆ° auto_mapï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¤„ç†")
                
        except Exception as e:
            print(f"[DEBUG] âš ï¸  é…ç½®æ£€æŸ¥å¤±è´¥ï¼ˆç»§ç»­å°è¯•åŠ è½½ï¼‰: {e}")
            import traceback
            traceback.print_exc()
        
        # å°è¯•å¤šç§åŠ è½½æ–¹å¼
        print("\n[DEBUG] æ­¥éª¤2: å°è¯•åŠ è½½æ¨¡å‹...")
        
        # æ–¹æ³•1: ä½¿ç”¨ AutoModel + trust_remote_codeï¼ˆç»•è¿‡é…ç½®æ£€æŸ¥ï¼‰
        print("[DEBUG] æ–¹æ³•1: ä½¿ç”¨ AutoModel.from_pretrained + trust_remote_code=True")
        print("[DEBUG] æ³¨æ„: å³ä½¿é…ç½®åŠ è½½å¤±è´¥ï¼Œä¹Ÿå°è¯•ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆtrust_remote_code åº”è¯¥ä¼šå¤„ç†è‡ªå®šä¹‰ä»£ç ï¼‰")
        try:
            # ç›´æ¥å°è¯•åŠ è½½ï¼Œè®© trust_remote_code å¤„ç†è‡ªå®šä¹‰ä»£ç 
            model = AutoModel.from_pretrained(
                MODEL_ID,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True,  # å…³é”®ï¼šè¿™ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
                local_files_only=False  # ç¡®ä¿ä»è¿œç¨‹ä¸‹è½½è‡ªå®šä¹‰ä»£ç 
            )
            print("[DEBUG] âœ… æ–¹æ³•1æˆåŠŸ: AutoModel åŠ è½½æˆåŠŸ")
        except Exception as e1:
            print(f"[DEBUG] âŒ æ–¹æ³•1å¤±è´¥: {type(e1).__name__}: {e1}")
            import traceback
            traceback.print_exc()
            
            # æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç 
            print("\n[DEBUG] æ–¹æ³•2: å°è¯•æ‰‹åŠ¨åŠ è½½è‡ªå®šä¹‰ä»£ç ...")
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
                from huggingface_hub import list_repo_files
                
                print("[DEBUG] åˆ—å‡ºæ¨¡å‹ä»“åº“æ–‡ä»¶...")
                repo_files = list_repo_files(
                    repo_id=MODEL_ID,
                    token=HF_TOKEN
                )
                print(f"[DEBUG] ä»“åº“æ–‡ä»¶: {[f for f in repo_files if '.py' in f]}")
                
                # æŸ¥æ‰¾ modeling æ–‡ä»¶
                modeling_files = [f for f in repo_files if 'modeling' in f.lower() and f.endswith('.py')]
                if modeling_files:
                    print(f"[DEBUG] æ‰¾åˆ°å»ºæ¨¡æ–‡ä»¶: {modeling_files}")
                    # å°è¯•æ‰‹åŠ¨ä¸‹è½½å¹¶å¯¼å…¥
                    for model_file in modeling_files:
                        try:
                            print(f"[DEBUG] å°è¯•ä¸‹è½½å¹¶å¯¼å…¥: {model_file}")
                            custom_path = hf_hub_download(
                                repo_id=MODEL_ID,
                                filename=model_file,
                                token=HF_TOKEN
                            )
                            print(f"[DEBUG] è‡ªå®šä¹‰ä»£ç è·¯å¾„: {custom_path}")
                            # è¿™é‡Œå¯ä»¥å°è¯•åŠ¨æ€å¯¼å…¥ï¼Œä½†æ¯”è¾ƒå¤æ‚
                        except Exception as e:
                            print(f"[DEBUG] ä¸‹è½½ {model_file} å¤±è´¥: {e}")
                
            except Exception as e2:
                print(f"[DEBUG] âš ï¸  æ–¹æ³•2å¤±è´¥: {e2}")
            
            # æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGenerationï¼ˆä½œä¸ºå›é€€ï¼‰
            print("\n[DEBUG] æ–¹æ³•3: å°è¯•ä½¿ç”¨ MoshiForConditionalGeneration...")
            try:
                from transformers import MoshiForConditionalGeneration
                model = MoshiForConditionalGeneration.from_pretrained(
                    MODEL_ID,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                print("[DEBUG] âœ… æ–¹æ³•3æˆåŠŸ: MoshiForConditionalGeneration åŠ è½½æˆåŠŸ")
            except Exception as e3:
                print(f"[DEBUG] âŒ æ–¹æ³•3å¤±è´¥: {type(e3).__name__}: {e3}")
                import traceback
                traceback.print_exc()
                
                # æœ€ç»ˆé”™è¯¯å¤„ç†
                error_msg = str(e1)
                return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

å½“å‰ Transformers ç‰ˆæœ¬: {transformers_version}
ä¸»è¦é”™è¯¯: {error_msg}

å·²å°è¯•çš„æ–¹æ³•:
1. AutoModel.from_pretrained + trust_remote_code=True
2. æ‰‹åŠ¨æ£€æŸ¥è‡ªå®šä¹‰ä»£ç æ–‡ä»¶
3. MoshiForConditionalGeneration

é—®é¢˜åˆ†æ:
PersonaPlex ä½¿ç”¨è‡ªå®šä¹‰æ¶æ„ï¼Œéœ€è¦ä»æ¨¡å‹ä»“åº“åŠ è½½è‡ªå®šä¹‰ä»£ç ã€‚
ä½† Transformers åœ¨åŠ è½½é…ç½®æ—¶å°±å¤±è´¥äº†ï¼Œæ— æ³•ç»§ç»­ã€‚

è§£å†³æ–¹æ¡ˆ:
ç”±äº PersonaPlex æ¶æ„å¤ªæ–°ï¼Œå½“å‰ Transformers ç‰ˆæœ¬å¯èƒ½è¿˜ä¸å®Œå…¨æ”¯æŒã€‚
å»ºè®®:
1. ç­‰å¾… Transformers æ›´æ–°æ”¯æŒ PersonaPlex
2. æˆ–ä½¿ç”¨å®˜æ–¹ PersonaPlex ä»£ç åº“: https://github.com/NVIDIA/personaplex
3. æˆ–æ‰‹åŠ¨å®ç°æ¨¡å‹åŠ è½½é€»è¾‘"""
                
                raise e1
        
        # éªŒè¯æ¨¡å‹
        print("\n[DEBUG] æ­¥éª¤3: éªŒè¯æ¨¡å‹...")
        if model is None:
            raise Exception("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodel ä¸º None")
        
        print(f"[DEBUG] æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"[DEBUG] æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
        
        model.eval()
        mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0
        model_status = "å·²åŠ è½½"
        
        print(f"\n[DEBUG] âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼æ˜¾å­˜: {mem:.2f} GB")
        print("="*60)
        
        return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼({mem:.2f} GB)"
        
    except Exception as e:
        model_status = "åŠ è½½å¤±è´¥"
        error_msg = str(e)
        error_type = type(e).__name__
        
        print(f"\n[DEBUG] âŒ æœ€ç»ˆé”™è¯¯: {error_type}: {error_msg}")
        import traceback
        traceback.print_exc()
        
        return f"""âŒ æ¨¡å‹åŠ è½½å¤±è´¥

é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯ä¿¡æ¯: {error_msg}

è¯·æŸ¥çœ‹æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚"""

def process_voice(audio):
    """å¤„ç†è¯­éŸ³"""
    global model
    
    if model is None:
        return "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹", "âŒ æ¨¡å‹æœªåŠ è½½"
    
    if audio is None:
        return "", ""
    
    try:
        # è¯»å–éŸ³é¢‘
        audio_data, sr = sf.read(audio)
        if len(audio_data.shape) > 1:
            audio_data = np.mean(audio_data, axis=1)
        
        # é‡é‡‡æ ·åˆ° 24kHz
        if sr != 24000:
            import librosa
            audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=24000)
            sr = 24000
        
        duration = len(audio_data) / sr
        user_text = f"ğŸ¤ è¯­éŸ³è¾“å…¥ ({duration:.2f}ç§’)"
        
        # å°è¯•è°ƒç”¨æ¨¡å‹ï¼ˆå³ä½¿æ²¡æœ‰processorï¼‰
        try:
            # å°†éŸ³é¢‘è½¬æ¢ä¸ºtensor
            audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0).to(device)
            
            # å°è¯•ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼ˆéœ€è¦æ ¹æ®å®é™…æ¶æ„è°ƒæ•´ï¼‰
            # ç”±äºPersonaPlexæ¶æ„ç‰¹æ®Šï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å°è¯•
            with torch.no_grad():
                # å°è¯•ä½¿ç”¨æ¨¡å‹çš„forwardæ–¹æ³•
                # æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼
                try:
                    # åˆ›å»ºä¸€ä¸ªç®€å•çš„è¾“å…¥ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
                    # PersonaPlexå¯èƒ½éœ€è¦audio codeså’Œtext tokens
                    # è¿™é‡Œæˆ‘ä»¬å°è¯•æœ€ç®€å•çš„è°ƒç”¨
                    
                    # ç”±äºæ²¡æœ‰processorï¼Œæˆ‘ä»¬æ— æ³•æ­£ç¡®ç¼–ç è¾“å…¥
                    # ä½†å¯ä»¥æ˜¾ç¤ºæ¨¡å‹å·²å‡†å¤‡å¥½
                    ai_text = f"âœ… å·²æ”¶åˆ°è¯­éŸ³ ({duration:.2f}ç§’)\n\næ¨¡å‹å·²åŠ è½½å¹¶å‡†å¤‡å¤„ç†ã€‚\n\nâš ï¸ ç”±äºç¼ºå°‘processorï¼Œæ— æ³•å®Œæˆå®Œæ•´æ¨ç†ã€‚\næ¨¡å‹éœ€è¦ç‰¹å®šçš„éŸ³é¢‘ç¼–ç æ ¼å¼ã€‚"
                    
                except Exception as e:
                    ai_text = f"âœ… æ¨¡å‹å·²åŠ è½½\n\nâš ï¸ æ¨ç†éœ€è¦processoræˆ–äº†è§£è¾“å…¥æ ¼å¼ã€‚\né”™è¯¯: {str(e)}"
        except Exception as e:
            ai_text = f"âœ… éŸ³é¢‘å·²å¤„ç†\n\nâš ï¸ æ¨¡å‹è°ƒç”¨éœ€è¦ç‰¹å®šæ ¼å¼ã€‚\n{str(e)}"
        
        return user_text, ai_text
        
    except Exception as e:
        return f"âŒ é”™è¯¯: {str(e)}", ""

# å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
def auto_load_model():
    """ç¨‹åºå¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹"""
    print("="*60)
    print("è‡ªåŠ¨åŠ è½½æ¨¡å‹...")
    print("="*60)
    return load_model()

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="PersonaPlex è¯­éŸ³å¯¹è¯", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ™ï¸ PersonaPlex è¯­éŸ³å¯¹è¯")
    
    # çŠ¶æ€æ˜¾ç¤ºï¼ˆè‡ªåŠ¨åŠ è½½ï¼‰
    status = gr.Textbox(
        label="æ¨¡å‹çŠ¶æ€", 
        value="æ­£åœ¨åŠ è½½æ¨¡å‹...", 
        interactive=False,
        lines=3
    )
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½æŒ‰é’®ï¼ˆå¯é€‰ï¼‰
    load_btn = gr.Button("ğŸ”„ é‡æ–°åŠ è½½æ¨¡å‹", variant="secondary", size="sm")
    
    gr.Markdown("---")
    
    # å¯¹è¯åŒºåŸŸ
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ‘¤ æ‚¨è¯´çš„è¯")
            user_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
        
        with gr.Column():
            gr.Markdown("### ğŸ¤– AI å›å¤")
            ai_text = gr.Textbox(label="", lines=12, interactive=False, placeholder="...")
    
    # æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰
    text_prompt_input = gr.Textbox(
        label="æ–‡æœ¬æç¤ºï¼ˆå¯é€‰ï¼‰",
        placeholder="ä¾‹å¦‚: You are a helpful AI assistant.",
        lines=2,
        value="You are a helpful AI assistant. Respond naturally."
    )
    
    # è¯­éŸ³è¾“å…¥
    audio_input = gr.Audio(
        label="",
        type="filepath",
        sources=["microphone"],
        format="wav",
        show_label=False
    )
    
    # äº‹ä»¶
    # å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹
    demo.load(fn=auto_load_model, outputs=status)
    
    # æ‰‹åŠ¨é‡æ–°åŠ è½½
    load_btn.click(fn=load_model, outputs=status)
    
    # è¯­éŸ³å¤„ç†ï¼ˆåŒ…å«æ–‡æœ¬æç¤ºï¼‰
    audio_input.change(
        fn=process_voice,
        inputs=[audio_input, text_prompt_input],
        outputs=[user_text, ai_text]
    )
    
    # å¯é€‰ï¼šå¦‚æœæ–‡æœ¬æç¤ºæ”¹å˜ï¼Œä¹Ÿè§¦å‘å¤„ç†ï¼ˆå¦‚æœéŸ³é¢‘å·²å­˜åœ¨ï¼‰
    # text_prompt_input.change(
    #     fn=lambda prompt, audio: process_voice(audio, prompt) if audio else ("", ""),
    #     inputs=[text_prompt_input, audio_input],
    #     outputs=[user_text, ai_text]
    # )

if __name__ == "__main__":
    print("="*60)
    print("PersonaPlex è¯­éŸ³å¯¹è¯ - ç«¯å£ 5001")
    print("="*60)
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=5001,
        share=False
    )

